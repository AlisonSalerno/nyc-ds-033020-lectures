{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='binary:logistic', random_state=None, reg_alpha=None,\n",
       "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "              tree_method=None, validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 4, \n",
    "                           alpha = 1, \n",
    "                           scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=10000, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=1,\n",
       "              reg_lambda=1, scale_pos_weight=0.38245219347581555, subsample=0.5,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.676471\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 3, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.659772</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.660168</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639740</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.640184</td>\n",
       "      <td>0.010529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.627790</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>0.010380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.614868</td>\n",
       "      <td>0.010607</td>\n",
       "      <td>0.616503</td>\n",
       "      <td>0.015193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.599252</td>\n",
       "      <td>0.010671</td>\n",
       "      <td>0.601503</td>\n",
       "      <td>0.014880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.372616</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.434313</td>\n",
       "      <td>0.038028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.371920</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.434456</td>\n",
       "      <td>0.038454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.371195</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.434110</td>\n",
       "      <td>0.038592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.370798</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.434108</td>\n",
       "      <td>0.038613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.370584</td>\n",
       "      <td>0.009246</td>\n",
       "      <td>0.433982</td>\n",
       "      <td>0.038632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.659772           0.000848           0.660168   \n",
       "1              0.639740           0.006761           0.640184   \n",
       "2              0.627790           0.006071           0.629019   \n",
       "3              0.614868           0.010607           0.616503   \n",
       "4              0.599252           0.010671           0.601503   \n",
       "..                  ...                ...                ...   \n",
       "127            0.372616           0.009112           0.434313   \n",
       "128            0.371920           0.009004           0.434456   \n",
       "129            0.371195           0.009250           0.434110   \n",
       "130            0.370798           0.009382           0.434108   \n",
       "131            0.370584           0.009246           0.433982   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.001478  \n",
       "1            0.010529  \n",
       "2            0.010380  \n",
       "3            0.015193  \n",
       "4            0.014880  \n",
       "..                ...  \n",
       "127          0.038028  \n",
       "128          0.038454  \n",
       "129          0.038592  \n",
       "130          0.038613  \n",
       "131          0.038632  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8919\n",
      "AUC Score (Train): 0.939281\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic')\n",
    "param_dist = {'n_estimators': [100,300,500],\n",
    "              'learning_rate': [0.1,0.07,0.05,0.03,0.01],\n",
    "              'max_depth': [3, 4, 5, 6, 7],\n",
    "              'colsample_bytree': [0.5,0.45,0.4],\n",
    "              'min_child_weight': [1, 2, 3]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = clf_xgb,\n",
    "    param_grid = param_dist, \n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 675 candidates, totalling 3375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   31.6s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   52.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3375 out of 3375 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constrai...\n",
       "                                     reg_lambda=None, scale_pos_weight=None,\n",
       "                                     subsample=None, tree_method=None,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             iid=False, n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5, 0.45, 0.4],\n",
       "                         'learning_rate': [0.1, 0.07, 0.05, 0.03, 0.01],\n",
       "                         'max_depth': [3, 4, 5, 6, 7],\n",
       "                         'min_child_weight': [1, 2, 3],\n",
       "                         'n_estimators': [100, 300, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06495547, 0.16774845, 0.27859693, 0.05224433, 0.16209555,\n",
       "        0.26952243, 0.05842242, 0.20006609, 0.30110383, 0.0957962 ,\n",
       "        0.20577683, 0.3275435 , 0.07616205, 0.19218941, 0.30939837,\n",
       "        0.06941991, 0.21945658, 0.35671172, 0.10839338, 0.25522833,\n",
       "        0.43147058, 0.10929432, 0.23181391, 0.38365088, 0.07793088,\n",
       "        0.19661889, 0.35987377, 0.10403428, 0.26712518, 0.43463755,\n",
       "        0.08406167, 0.24919319, 0.40684786, 0.0795074 , 0.22356868,\n",
       "        0.4055366 , 0.09231143, 0.24586349, 0.45845251, 0.11358604,\n",
       "        0.2549921 , 0.39762344, 0.10823722, 0.23921533, 0.42460961,\n",
       "        0.06516223, 0.16045823, 0.27614956, 0.07067633, 0.1915164 ,\n",
       "        0.271071  , 0.06561546, 0.17597933, 0.33305211, 0.07822189,\n",
       "        0.21359081, 0.3729269 , 0.08852062, 0.20486798, 0.3354291 ,\n",
       "        0.07357378, 0.18997321, 0.30160213, 0.07806139, 0.2543066 ,\n",
       "        0.33938518, 0.0852819 , 0.21509399, 0.35484481, 0.07558675,\n",
       "        0.21520901, 0.35318551, 0.08515215, 0.25278416, 0.39316225,\n",
       "        0.09179635, 0.24726596, 0.40007372, 0.09810643, 0.24297862,\n",
       "        0.41863236, 0.10820069, 0.3028512 , 0.47324343, 0.10051928,\n",
       "        0.24895015, 0.42162676, 0.09035797, 0.25240998, 0.44234066,\n",
       "        0.06998339, 0.15899611, 0.2721242 , 0.05869923, 0.17219939,\n",
       "        0.27048607, 0.05487065, 0.17562127, 0.26743455, 0.0741045 ,\n",
       "        0.19677887, 0.33863225, 0.07209244, 0.18983159, 0.32540231,\n",
       "        0.07278132, 0.20435352, 0.33586144, 0.08455682, 0.22940874,\n",
       "        0.40154362, 0.0928822 , 0.22631121, 0.36835546, 0.09532704,\n",
       "        0.24368434, 0.38104358, 0.09201932, 0.2542666 , 0.39965167,\n",
       "        0.09056439, 0.2515234 , 0.40681071, 0.09311881, 0.25101147,\n",
       "        0.50231867, 0.1035295 , 0.31570411, 0.49374356, 0.10338035,\n",
       "        0.2742238 , 0.49508719, 0.10651722, 0.29736691, 0.44937139,\n",
       "        0.05968056, 0.16942463, 0.26252966, 0.06912255, 0.16031008,\n",
       "        0.26071882, 0.06064496, 0.16343265, 0.2767458 , 0.07125645,\n",
       "        0.19750104, 0.30590339, 0.08056374, 0.20327244, 0.3472218 ,\n",
       "        0.0752172 , 0.19694309, 0.33546619, 0.08511724, 0.2317894 ,\n",
       "        0.37720571, 0.08359537, 0.24023342, 0.37216835, 0.10129342,\n",
       "        0.23941121, 0.38067622, 0.09498096, 0.27549505, 0.41481338,\n",
       "        0.09044509, 0.2543726 , 0.41518283, 0.08838725, 0.2748137 ,\n",
       "        0.4283175 , 0.10429773, 0.35568066, 0.55147305, 0.10593061,\n",
       "        0.28510203, 0.46221914, 0.08923106, 0.26015201, 0.44434261,\n",
       "        0.07666073, 0.15183382, 0.26768966, 0.0631732 , 0.18120012,\n",
       "        0.26413703, 0.07984872, 0.17055326, 0.29716849, 0.07584758,\n",
       "        0.19733849, 0.3501152 , 0.09036584, 0.20083404, 0.33655658,\n",
       "        0.07654395, 0.20912519, 0.32495928, 0.08558044, 0.22834935,\n",
       "        0.39672718, 0.08607106, 0.24810534, 0.40271258, 0.08057518,\n",
       "        0.23366556, 0.38647442, 0.10171914, 0.25592117, 0.42109222,\n",
       "        0.09574146, 0.25745816, 0.41178417, 0.09327111, 0.26104326,\n",
       "        0.42072511, 0.09564104, 0.28942981, 0.47677531, 0.09356217,\n",
       "        0.28603568, 0.47294412, 0.10024204, 0.26191602, 0.46129441,\n",
       "        0.07363195, 0.16575809, 0.27607002, 0.06277065, 0.17104378,\n",
       "        0.26953878, 0.06724815, 0.16864524, 0.26593199, 0.08190546,\n",
       "        0.20518579, 0.32761812, 0.0912797 , 0.19826765, 0.33433757,\n",
       "        0.07161846, 0.21381855, 0.32085915, 0.08823442, 0.23635626,\n",
       "        0.38833694, 0.08399944, 0.22226586, 0.39914398, 0.08869638,\n",
       "        0.2107986 , 0.35595803, 0.09376497, 0.26133351, 0.43572316,\n",
       "        0.08814526, 0.24679613, 0.40796728, 0.08977733, 0.24605336,\n",
       "        0.39160228, 0.09710355, 0.27439427, 0.45537596, 0.09857779,\n",
       "        0.26869078, 0.4400043 , 0.09866519, 0.25573444, 0.43278213,\n",
       "        0.06749043, 0.16406708, 0.29935632, 0.06958089, 0.16898637,\n",
       "        0.28651409, 0.06508365, 0.1879652 , 0.285741  , 0.08408494,\n",
       "        0.21156802, 0.35922127, 0.0785223 , 0.2105834 , 0.33663535,\n",
       "        0.0728528 , 0.20780826, 0.34014287, 0.08293557, 0.24103336,\n",
       "        0.39844189, 0.08418498, 0.22439461, 0.39763694, 0.09389215,\n",
       "        0.22358394, 0.37466545, 0.10634999, 0.27052894, 0.42861853,\n",
       "        0.09135776, 0.2420362 , 0.42361422, 0.09966121, 0.24648333,\n",
       "        0.38751273, 0.10952463, 0.28634186, 0.52103782, 0.11895375,\n",
       "        0.30576787, 0.52026734, 0.09810266, 0.27173657, 0.42861018,\n",
       "        0.05991826, 0.16223707, 0.28401942, 0.0675385 , 0.1794498 ,\n",
       "        0.27438817, 0.06816864, 0.17056351, 0.29185076, 0.07946396,\n",
       "        0.21277118, 0.33818502, 0.0759151 , 0.20943494, 0.34173021,\n",
       "        0.07294626, 0.21532106, 0.35211401, 0.08588104, 0.23290138,\n",
       "        0.41120572, 0.08980913, 0.25346518, 0.39478769, 0.08472905,\n",
       "        0.23204012, 0.39415946, 0.10202036, 0.25788202, 0.46281314,\n",
       "        0.09086938, 0.2543685 , 0.41327524, 0.0901988 , 0.25701075,\n",
       "        0.41291981, 0.10016804, 0.2876874 , 0.47007928, 0.10276446,\n",
       "        0.28276043, 0.45407705, 0.09400473, 0.26792202, 0.46142478,\n",
       "        0.06850882, 0.16978092, 0.29845924, 0.06674476, 0.17721586,\n",
       "        0.27716203, 0.07157445, 0.17949648, 0.28462353, 0.0751862 ,\n",
       "        0.21395788, 0.33227038, 0.07537246, 0.19543447, 0.34098849,\n",
       "        0.07149849, 0.19725485, 0.34392529, 0.08980556, 0.24952183,\n",
       "        0.39923677, 0.08509226, 0.23132062, 0.37994423, 0.08848562,\n",
       "        0.22161059, 0.37818241, 0.10796185, 0.25895391, 0.45292778,\n",
       "        0.10580163, 0.27707896, 0.45502276, 0.09057541, 0.29301305,\n",
       "        0.49532957, 0.12954626, 0.35700741, 0.89783626, 0.31595497,\n",
       "        0.61017742, 0.52640281, 0.11072111, 0.33861599, 0.63521843,\n",
       "        0.10534129, 0.21333704, 0.39510107, 0.10284753, 0.20184436,\n",
       "        0.36064763, 0.0681582 , 0.18120542, 0.41315155, 0.14985852,\n",
       "        0.34089479, 0.45291357, 0.08980432, 0.2980834 , 0.44509206,\n",
       "        0.09463181, 0.29557362, 0.42215004, 0.11661854, 0.25209427,\n",
       "        0.48464069, 0.10221667, 0.24901724, 0.49088202, 0.11338143,\n",
       "        0.28262477, 0.40838003, 0.1197083 , 0.32701397, 0.5024498 ,\n",
       "        0.09976134, 0.26943836, 0.45306506, 0.09898939, 0.2740654 ,\n",
       "        0.45193315, 0.1098258 , 0.30103259, 0.50344949, 0.09863963,\n",
       "        0.29283838, 0.47190824, 0.09386125, 0.27143579, 0.46132989,\n",
       "        0.0493052 , 0.15757437, 0.27390261, 0.06485453, 0.1582871 ,\n",
       "        0.26394539, 0.0589766 , 0.16432495, 0.29482961, 0.08162556,\n",
       "        0.19265375, 0.3210475 , 0.07598219, 0.23332996, 0.33034692,\n",
       "        0.06899476, 0.20778856, 0.31408944, 0.08677092, 0.21024594,\n",
       "        0.35892477, 0.08147745, 0.21411343, 0.34548302, 0.06993632,\n",
       "        0.20458236, 0.32678604, 0.08672695, 0.23395896, 0.39292336,\n",
       "        0.08517075, 0.23541503, 0.37422919, 0.0856626 , 0.21511998,\n",
       "        0.43999257, 0.19995804, 0.46242332, 0.47850943, 0.09253497,\n",
       "        0.21412849, 0.38584876, 0.08231359, 0.22258039, 0.36797905,\n",
       "        0.06549745, 0.31344867, 0.50003171, 0.11175718, 0.26383643,\n",
       "        0.43179765, 0.0690836 , 0.17631607, 0.30913796, 0.08724198,\n",
       "        0.21380839, 0.33762937, 0.08433452, 0.1944427 , 0.43740616,\n",
       "        0.12931752, 0.22766757, 0.3155478 , 0.08645844, 0.19965539,\n",
       "        0.34281673, 0.08240685, 0.2320334 , 0.32306895, 0.0733911 ,\n",
       "        0.2085187 , 0.38090291, 0.08626814, 0.25595603, 0.38894491,\n",
       "        0.08509622, 0.23014922, 0.36178956, 0.08332286, 0.22809825,\n",
       "        0.34490528, 0.0877646 , 0.22815943, 0.44688053, 0.08786259,\n",
       "        0.21268597, 0.36114082, 0.07989578, 0.23615403, 0.38117566,\n",
       "        0.06777277, 0.16508636, 0.26212063, 0.0657228 , 0.17093258,\n",
       "        0.26674199, 0.05948448, 0.17350011, 0.29607859, 0.06504159,\n",
       "        0.2068522 , 0.3059454 , 0.07953196, 0.19524693, 0.31938162,\n",
       "        0.06821809, 0.19234138, 0.33005624, 0.08087482, 0.21644611,\n",
       "        0.3708746 , 0.0765224 , 0.21911922, 0.37922888, 0.08195925,\n",
       "        0.20555477, 0.39545879, 0.08047481, 0.23710361, 0.3781764 ,\n",
       "        0.0816761 , 0.23857617, 0.35145822, 0.08200884, 0.21976156,\n",
       "        0.3454989 , 0.09041982, 0.251262  , 0.42346287, 0.09492922,\n",
       "        0.24933548, 0.40697823, 0.08432331, 0.23961678, 0.39582639,\n",
       "        0.06751275, 0.15425358, 0.27892699, 0.06041512, 0.17760663,\n",
       "        0.26994395, 0.0608933 , 0.1781692 , 0.28683724, 0.07950683,\n",
       "        0.19884582, 0.32124467, 0.06875882, 0.18407235, 0.31314578,\n",
       "        0.08395386, 0.1971046 , 0.32289338, 0.09222527, 0.21816115,\n",
       "        0.33193474, 0.07624712, 0.20451035, 0.34235721, 0.07230253,\n",
       "        0.21451745, 0.32788963, 0.09020057, 0.22589469, 0.36770077,\n",
       "        0.090376  , 0.22906032, 0.41339622, 0.09512992, 0.23108025,\n",
       "        0.37539263, 0.10044889, 0.25017776, 0.41658268, 0.08648939,\n",
       "        0.24044743, 0.40767722, 0.08471231, 0.2326498 , 0.41690326,\n",
       "        0.06168423, 0.16834497, 0.27664905, 0.06519938, 0.16831331,\n",
       "        0.27303042, 0.05606961, 0.15964074, 0.2798171 , 0.0815062 ,\n",
       "        0.19857049, 0.31665797, 0.07022319, 0.19370451, 0.31663752,\n",
       "        0.07201505, 0.171452  , 0.30341692, 0.08386235, 0.22637887,\n",
       "        0.35002813, 0.07845044, 0.23724036, 0.37854443, 0.07420082,\n",
       "        0.20818305, 0.32406049, 0.0839889 , 0.24679966, 0.37575765,\n",
       "        0.07715354, 0.22817354, 0.4018425 , 0.08177776, 0.22069197,\n",
       "        0.37050052, 0.10368476, 0.24886632, 0.42080245, 0.08707623,\n",
       "        0.25596638, 0.41730151, 0.09159765, 0.25442781, 0.37504892]),\n",
       " 'std_fit_time': array([0.00203169, 0.00202298, 0.01679911, 0.00619792, 0.00875166,\n",
       "        0.00296666, 0.00841507, 0.01594505, 0.02979548, 0.01273496,\n",
       "        0.02733037, 0.02312904, 0.01563908, 0.01413654, 0.00584668,\n",
       "        0.00536714, 0.02160186, 0.02325176, 0.00775498, 0.0109649 ,\n",
       "        0.08350697, 0.01655956, 0.03067772, 0.03328082, 0.00398723,\n",
       "        0.01682756, 0.02597464, 0.01105828, 0.04296593, 0.02336865,\n",
       "        0.00337914, 0.02000828, 0.03565681, 0.00454481, 0.00721774,\n",
       "        0.03286931, 0.00606319, 0.01965855, 0.02407972, 0.0154942 ,\n",
       "        0.01287318, 0.02405583, 0.01361927, 0.01098841, 0.01559196,\n",
       "        0.00317112, 0.0060798 , 0.02046759, 0.01411829, 0.04411067,\n",
       "        0.02754365, 0.00509507, 0.01330966, 0.03879844, 0.01107605,\n",
       "        0.01661696, 0.02852518, 0.00865339, 0.00366136, 0.01066226,\n",
       "        0.00427904, 0.01449147, 0.01109316, 0.00339393, 0.03530936,\n",
       "        0.0121264 , 0.01215686, 0.01090608, 0.01614142, 0.01263665,\n",
       "        0.01126766, 0.00300782, 0.00412517, 0.03262806, 0.04272231,\n",
       "        0.00567095, 0.01122734, 0.02090423, 0.01056834, 0.0056513 ,\n",
       "        0.02680733, 0.00462425, 0.02971998, 0.00863707, 0.01039802,\n",
       "        0.00604496, 0.0121598 , 0.0067632 , 0.01095219, 0.01711349,\n",
       "        0.00902736, 0.0027505 , 0.0114962 , 0.00238988, 0.00659162,\n",
       "        0.02298363, 0.00754075, 0.01087473, 0.01591766, 0.00406364,\n",
       "        0.01081886, 0.0162656 , 0.00686387, 0.00910947, 0.01968275,\n",
       "        0.0052299 , 0.01557023, 0.01152759, 0.01232524, 0.00653594,\n",
       "        0.03029777, 0.01929789, 0.01792005, 0.02548823, 0.00605651,\n",
       "        0.01058842, 0.01263926, 0.00288802, 0.00927337, 0.01357206,\n",
       "        0.00976753, 0.00767173, 0.01786   , 0.01243424, 0.01812816,\n",
       "        0.02647933, 0.03000672, 0.01371255, 0.01739571, 0.00793649,\n",
       "        0.01839982, 0.02590732, 0.02903073, 0.01681983, 0.03128183,\n",
       "        0.00635087, 0.00902886, 0.00972158, 0.01020111, 0.0101594 ,\n",
       "        0.02404411, 0.00569936, 0.01466605, 0.01908995, 0.00405311,\n",
       "        0.00624915, 0.01863616, 0.00702049, 0.00663152, 0.01965297,\n",
       "        0.00722841, 0.00715429, 0.02258554, 0.00886018, 0.01438836,\n",
       "        0.00353837, 0.00599771, 0.00769427, 0.0233318 , 0.01134996,\n",
       "        0.00485009, 0.00875068, 0.00292338, 0.01473036, 0.01590068,\n",
       "        0.00657776, 0.01048143, 0.04555505, 0.01036204, 0.03266373,\n",
       "        0.02875173, 0.00678573, 0.07855155, 0.05863783, 0.01204618,\n",
       "        0.03309433, 0.05000672, 0.00431163, 0.01560311, 0.00317549,\n",
       "        0.01247413, 0.01070937, 0.00706384, 0.0044092 , 0.00769   ,\n",
       "        0.018005  , 0.03016165, 0.01712903, 0.00630647, 0.00845489,\n",
       "        0.01629457, 0.02620749, 0.00638665, 0.01330048, 0.00994677,\n",
       "        0.00331703, 0.0197129 , 0.01381692, 0.00505953, 0.00879486,\n",
       "        0.01596027, 0.00527462, 0.01326309, 0.01118406, 0.0054814 ,\n",
       "        0.0169186 , 0.02609154, 0.00476106, 0.01452216, 0.01590321,\n",
       "        0.00751437, 0.01367243, 0.01030795, 0.0047066 , 0.01772334,\n",
       "        0.00801813, 0.00481852, 0.0118314 , 0.00891449, 0.00598503,\n",
       "        0.01135809, 0.0069025 , 0.00632689, 0.01130226, 0.04610202,\n",
       "        0.01337423, 0.00228105, 0.01102883, 0.00280192, 0.00415973,\n",
       "        0.01596358, 0.00559337, 0.00898625, 0.03160685, 0.00807692,\n",
       "        0.00672672, 0.01947495, 0.01372862, 0.01928464, 0.01285512,\n",
       "        0.00552269, 0.00708187, 0.00819292, 0.00338336, 0.01127844,\n",
       "        0.01192298, 0.00259817, 0.01808374, 0.01209669, 0.0105082 ,\n",
       "        0.00534124, 0.01115401, 0.00419849, 0.01390499, 0.02487732,\n",
       "        0.00431289, 0.00634905, 0.0066563 , 0.00869099, 0.00659254,\n",
       "        0.00596858, 0.00400017, 0.00884212, 0.01349814, 0.00346447,\n",
       "        0.01001824, 0.01141649, 0.00844464, 0.01571898, 0.02441979,\n",
       "        0.0052917 , 0.00264595, 0.02851038, 0.00953639, 0.00898578,\n",
       "        0.01169451, 0.00643236, 0.01045451, 0.00775677, 0.00319824,\n",
       "        0.01164469, 0.01847667, 0.00730174, 0.01845471, 0.00674516,\n",
       "        0.00909248, 0.01569279, 0.0131357 , 0.00927127, 0.02460384,\n",
       "        0.01962452, 0.00873298, 0.01524529, 0.02447912, 0.00341071,\n",
       "        0.02541301, 0.03010856, 0.01646716, 0.03225747, 0.02581765,\n",
       "        0.00857049, 0.01182866, 0.02232957, 0.00958717, 0.01113381,\n",
       "        0.01839789, 0.01189078, 0.01279045, 0.03463909, 0.01446005,\n",
       "        0.03757419, 0.04004632, 0.01324338, 0.00780793, 0.01953624,\n",
       "        0.00872362, 0.00380221, 0.01064066, 0.00947002, 0.00925438,\n",
       "        0.01269794, 0.00781119, 0.00727811, 0.00809386, 0.00143582,\n",
       "        0.021286  , 0.05889163, 0.00527788, 0.01598222, 0.02651262,\n",
       "        0.01071485, 0.02703257, 0.01228011, 0.00602776, 0.00909927,\n",
       "        0.04426402, 0.00940958, 0.01008653, 0.00363558, 0.0053524 ,\n",
       "        0.00877231, 0.01330792, 0.01093389, 0.01941254, 0.00762433,\n",
       "        0.00254108, 0.00949791, 0.01865513, 0.0042012 , 0.01018924,\n",
       "        0.01510616, 0.00535773, 0.00464439, 0.01835381, 0.00763985,\n",
       "        0.00792961, 0.01382659, 0.00673318, 0.01955213, 0.0164875 ,\n",
       "        0.00835051, 0.00608924, 0.01223085, 0.01048342, 0.00529805,\n",
       "        0.01880153, 0.00248467, 0.01092589, 0.00840186, 0.00337522,\n",
       "        0.01187275, 0.01153486, 0.00353375, 0.02151439, 0.00952552,\n",
       "        0.00789759, 0.0143762 , 0.02705882, 0.01400973, 0.0142347 ,\n",
       "        0.01288617, 0.00957781, 0.00561328, 0.02190359, 0.0066133 ,\n",
       "        0.00645129, 0.02690584, 0.00964637, 0.00996519, 0.02525498,\n",
       "        0.00869911, 0.01188383, 0.0098893 , 0.00552786, 0.02362755,\n",
       "        0.03316092, 0.01447722, 0.01631706, 0.30745642, 0.04032524,\n",
       "        0.14169664, 0.01140645, 0.00871471, 0.04267508, 0.05825649,\n",
       "        0.02600203, 0.01383403, 0.02624833, 0.02697234, 0.01912856,\n",
       "        0.0287314 , 0.00585574, 0.00523672, 0.12341885, 0.01800053,\n",
       "        0.08119758, 0.0203322 , 0.00706875, 0.03930619, 0.04507131,\n",
       "        0.00860044, 0.01951172, 0.02059149, 0.01416193, 0.02637247,\n",
       "        0.01568723, 0.00869106, 0.01501188, 0.02887745, 0.01254223,\n",
       "        0.02507394, 0.02892228, 0.02450211, 0.0197745 , 0.03942115,\n",
       "        0.00298077, 0.00666857, 0.01135989, 0.007611  , 0.01292871,\n",
       "        0.00909718, 0.00859509, 0.00753841, 0.02193114, 0.00545149,\n",
       "        0.00758918, 0.0112404 , 0.00277833, 0.01172193, 0.03658585,\n",
       "        0.00476499, 0.01812102, 0.00996353, 0.00480064, 0.00730397,\n",
       "        0.00772595, 0.0051754 , 0.01144279, 0.01090277, 0.01173176,\n",
       "        0.00528344, 0.01127858, 0.0127278 , 0.01282389, 0.00930274,\n",
       "        0.00928412, 0.01536273, 0.01570406, 0.00405824, 0.01196168,\n",
       "        0.02382976, 0.00962296, 0.05406449, 0.0059467 , 0.00573568,\n",
       "        0.01649619, 0.02298133, 0.01062762, 0.00192706, 0.02831133,\n",
       "        0.00602995, 0.01716707, 0.04479575, 0.01603572, 0.00489133,\n",
       "        0.12779428, 0.06304541, 0.04191878, 0.05047372, 0.02022156,\n",
       "        0.01023738, 0.0142225 , 0.00475955, 0.019938  , 0.01226289,\n",
       "        0.00460934, 0.16090557, 0.1171525 , 0.04447564, 0.0452053 ,\n",
       "        0.04900975, 0.00316196, 0.00933291, 0.02184185, 0.02213806,\n",
       "        0.01918919, 0.02490597, 0.00878828, 0.0131431 , 0.08627471,\n",
       "        0.02780488, 0.03861975, 0.01282393, 0.01550366, 0.01186346,\n",
       "        0.01832547, 0.00914742, 0.01663461, 0.0260121 , 0.00885684,\n",
       "        0.00334633, 0.00621123, 0.00725957, 0.01402913, 0.00701389,\n",
       "        0.0069737 , 0.00617538, 0.01987999, 0.01016777, 0.00756172,\n",
       "        0.03015063, 0.00565262, 0.00450398, 0.03331404, 0.01015914,\n",
       "        0.01300336, 0.01396448, 0.00855439, 0.00727068, 0.05152206,\n",
       "        0.0135392 , 0.01239053, 0.01943298, 0.00398446, 0.00777033,\n",
       "        0.04563405, 0.00628888, 0.01143408, 0.01236243, 0.00592568,\n",
       "        0.01415066, 0.02815402, 0.0137758 , 0.00858641, 0.00781021,\n",
       "        0.00640966, 0.0203113 , 0.01226067, 0.00894909, 0.03291133,\n",
       "        0.02647754, 0.00650127, 0.02228318, 0.02223544, 0.01277313,\n",
       "        0.01376499, 0.02920573, 0.00348439, 0.0152728 , 0.00778703,\n",
       "        0.00683761, 0.01256007, 0.01214525, 0.00602586, 0.00673935,\n",
       "        0.01044262, 0.00296752, 0.01561868, 0.01911731, 0.01317143,\n",
       "        0.0218049 , 0.01658828, 0.00391461, 0.02363966, 0.01087474,\n",
       "        0.00694939, 0.00843871, 0.01760718, 0.00416765, 0.00715808,\n",
       "        0.00759895, 0.00297097, 0.00815426, 0.01607453, 0.00911151,\n",
       "        0.01233708, 0.02069647, 0.01122458, 0.01979906, 0.03209878,\n",
       "        0.01184825, 0.01269173, 0.04117231, 0.00907144, 0.00555997,\n",
       "        0.00304232, 0.00786788, 0.01009689, 0.01317369, 0.00418683,\n",
       "        0.00753608, 0.033586  , 0.01156828, 0.0046639 , 0.02609601,\n",
       "        0.02100823, 0.01651156, 0.01061638, 0.03095345, 0.01018308,\n",
       "        0.01317327, 0.01074441, 0.00752367, 0.01405626, 0.00574343,\n",
       "        0.0116648 , 0.01182565, 0.00635924, 0.010518  , 0.02535693,\n",
       "        0.00828309, 0.00678843, 0.00888391, 0.01095168, 0.00590403,\n",
       "        0.00818419, 0.00486171, 0.00573584, 0.02052855, 0.01838538,\n",
       "        0.01709338, 0.00621938, 0.00944893, 0.01183709, 0.01679058,\n",
       "        0.00288931, 0.01286867, 0.00718478, 0.00893871, 0.01340693,\n",
       "        0.0086122 , 0.00498916, 0.00772828, 0.01752341, 0.01373079,\n",
       "        0.03740131, 0.01043571, 0.00752614, 0.01993409, 0.01655392,\n",
       "        0.00953478, 0.01750152, 0.02132087, 0.00732786, 0.00469161,\n",
       "        0.02548706, 0.01061201, 0.00704434, 0.01464269, 0.00655141,\n",
       "        0.01322004, 0.01597836, 0.00859992, 0.02845131, 0.07803006]),\n",
       " 'mean_score_time': array([0.01336956, 0.00764775, 0.00860748, 0.00761218, 0.00735168,\n",
       "        0.00820127, 0.00874352, 0.00871725, 0.01047821, 0.01143823,\n",
       "        0.00696321, 0.00989194, 0.0080142 , 0.0077611 , 0.00853581,\n",
       "        0.00649209, 0.0109971 , 0.01297145, 0.01475773, 0.01139064,\n",
       "        0.01169043, 0.00561285, 0.00798006, 0.01371555, 0.00675735,\n",
       "        0.01002879, 0.00938058, 0.00700412, 0.01204352, 0.01236634,\n",
       "        0.00718098, 0.00799885, 0.01007018, 0.00458617, 0.00791821,\n",
       "        0.0118504 , 0.00822816, 0.00874801, 0.0117959 , 0.00624013,\n",
       "        0.010952  , 0.01321082, 0.0068984 , 0.00761075, 0.01181812,\n",
       "        0.00599518, 0.00704169, 0.01118684, 0.00509281, 0.00855722,\n",
       "        0.00815444, 0.00632257, 0.00680084, 0.01034818, 0.00577879,\n",
       "        0.00734887, 0.01139145, 0.00789256, 0.01030974, 0.00967164,\n",
       "        0.00697165, 0.00605073, 0.01271782, 0.00769978, 0.00638547,\n",
       "        0.00840535, 0.00648255, 0.01042323, 0.00780563, 0.00708241,\n",
       "        0.00661349, 0.00995917, 0.00418458, 0.0065556 , 0.012679  ,\n",
       "        0.00479565, 0.00950742, 0.01208167, 0.0065042 , 0.01130257,\n",
       "        0.01209345, 0.00704861, 0.00875139, 0.01066618, 0.00669484,\n",
       "        0.00848403, 0.00921736, 0.00752087, 0.01209335, 0.01335583,\n",
       "        0.00697002, 0.00548239, 0.01071463, 0.00634007, 0.00875702,\n",
       "        0.01129003, 0.00715494, 0.00737095, 0.01044817, 0.01218557,\n",
       "        0.00723715, 0.00832276, 0.00590501, 0.00821781, 0.01071739,\n",
       "        0.00801206, 0.00860319, 0.00877614, 0.00872316, 0.0080121 ,\n",
       "        0.01035857, 0.0069387 , 0.00649972, 0.01188407, 0.00720592,\n",
       "        0.00790062, 0.01214204, 0.00597048, 0.01022062, 0.0131309 ,\n",
       "        0.00572095, 0.00884442, 0.01542268, 0.00665655, 0.0076869 ,\n",
       "        0.01237216, 0.0074935 , 0.00984669, 0.01567183, 0.00505438,\n",
       "        0.01054578, 0.01339951, 0.00735064, 0.01106358, 0.01083188,\n",
       "        0.00639615, 0.00812812, 0.00946875, 0.00540028, 0.0072289 ,\n",
       "        0.00859976, 0.00607362, 0.0074903 , 0.00783048, 0.00632687,\n",
       "        0.00765419, 0.01130028, 0.00874515, 0.01166811, 0.00907335,\n",
       "        0.006212  , 0.01128144, 0.0123631 , 0.00696292, 0.00774264,\n",
       "        0.01150975, 0.00658064, 0.00892081, 0.01456523, 0.00930405,\n",
       "        0.00816941, 0.00960288, 0.00723028, 0.0070076 , 0.01209979,\n",
       "        0.00630951, 0.01038728, 0.01217866, 0.00716677, 0.00909858,\n",
       "        0.01322999, 0.00515127, 0.01114516, 0.01034708, 0.00812502,\n",
       "        0.00916181, 0.01137533, 0.00632057, 0.00871162, 0.01329904,\n",
       "        0.00404315, 0.00928416, 0.00688615, 0.00509105, 0.00750685,\n",
       "        0.01112852, 0.00851583, 0.006989  , 0.0098856 , 0.00671201,\n",
       "        0.0072032 , 0.01369939, 0.00708051, 0.01004591, 0.01491466,\n",
       "        0.0063211 , 0.00702066, 0.0098382 , 0.0055397 , 0.01060634,\n",
       "        0.00913081, 0.00629859, 0.00730801, 0.00955334, 0.00615387,\n",
       "        0.00780101, 0.0104228 , 0.00548759, 0.00959535, 0.0151876 ,\n",
       "        0.00642614, 0.01101389, 0.01120443, 0.00630121, 0.01023183,\n",
       "        0.01041851, 0.00806236, 0.01024675, 0.01513963, 0.00704565,\n",
       "        0.00782275, 0.01280885, 0.00757499, 0.0091918 , 0.01477346,\n",
       "        0.00898104, 0.00746098, 0.00868988, 0.00546927, 0.00622258,\n",
       "        0.0125247 , 0.0053422 , 0.00724034, 0.01347404, 0.00638785,\n",
       "        0.00871601, 0.00907474, 0.00962501, 0.00985608, 0.0085093 ,\n",
       "        0.00796595, 0.00890207, 0.0076757 , 0.00637088, 0.00966926,\n",
       "        0.0132329 , 0.0087956 , 0.00951419, 0.01064677, 0.00701084,\n",
       "        0.0092165 , 0.01103902, 0.00771184, 0.0067853 , 0.01312766,\n",
       "        0.00501533, 0.00873446, 0.01199889, 0.00747981, 0.00722899,\n",
       "        0.01294932, 0.00637064, 0.00848269, 0.01231203, 0.00560045,\n",
       "        0.00745163, 0.01205158, 0.00755243, 0.007478  , 0.01050177,\n",
       "        0.00460315, 0.00552192, 0.00935812, 0.00654664, 0.00851889,\n",
       "        0.00820012, 0.0052577 , 0.00970616, 0.01025605, 0.00729547,\n",
       "        0.00738583, 0.01226044, 0.00594902, 0.00820012, 0.009584  ,\n",
       "        0.005549  , 0.00666418, 0.00969834, 0.00572705, 0.0076067 ,\n",
       "        0.01050358, 0.00539322, 0.00744057, 0.01211686, 0.00461779,\n",
       "        0.00674162, 0.01191616, 0.00896683, 0.01050591, 0.01315737,\n",
       "        0.00808415, 0.01004581, 0.01353078, 0.00817547, 0.01097407,\n",
       "        0.01247535, 0.00726962, 0.0081862 , 0.01338644, 0.00812278,\n",
       "        0.01191339, 0.01364722, 0.00470729, 0.00841031, 0.01025572,\n",
       "        0.00785289, 0.00598569, 0.01019988, 0.00635042, 0.00889902,\n",
       "        0.01043806, 0.00689573, 0.00799561, 0.00816998, 0.0095212 ,\n",
       "        0.00835824, 0.00944719, 0.00642138, 0.01142225, 0.0106523 ,\n",
       "        0.00674157, 0.00695472, 0.0121408 , 0.00710859, 0.0080874 ,\n",
       "        0.01293635, 0.00750933, 0.008605  , 0.01255007, 0.00481782,\n",
       "        0.0112042 , 0.01244535, 0.00969553, 0.00948539, 0.01325164,\n",
       "        0.00761905, 0.00883656, 0.01097484, 0.0056838 , 0.0091558 ,\n",
       "        0.01004009, 0.00648355, 0.00820222, 0.01106596, 0.00700307,\n",
       "        0.01339998, 0.01239491, 0.0050868 , 0.00871763, 0.01225162,\n",
       "        0.00552511, 0.0079237 , 0.00983157, 0.00736303, 0.00789585,\n",
       "        0.00811391, 0.00762601, 0.00829763, 0.00908594, 0.00592246,\n",
       "        0.0091598 , 0.01132469, 0.00719876, 0.00773315, 0.0082099 ,\n",
       "        0.0069262 , 0.00918493, 0.01603742, 0.00699115, 0.01195717,\n",
       "        0.01474199, 0.00930948, 0.01056304, 0.01079569, 0.00973811,\n",
       "        0.00784626, 0.01009283, 0.00926356, 0.00985565, 0.01462164,\n",
       "        0.00827961, 0.00837417, 0.01197844, 0.00720196, 0.00961914,\n",
       "        0.01773586, 0.0106657 , 0.00956874, 0.03122239, 0.02159681,\n",
       "        0.01146989, 0.01355219, 0.00963831, 0.01262565, 0.01982379,\n",
       "        0.00899596, 0.00959935, 0.01795664, 0.01656871, 0.00965862,\n",
       "        0.01242814, 0.00692573, 0.00823927, 0.02292385, 0.0257946 ,\n",
       "        0.00911846, 0.01852384, 0.00788507, 0.01618528, 0.01050277,\n",
       "        0.00812659, 0.0065114 , 0.01169972, 0.00790462, 0.00940619,\n",
       "        0.01750474, 0.00938897, 0.0089612 , 0.01312199, 0.01046958,\n",
       "        0.00982242, 0.01712046, 0.01129761, 0.01036386, 0.01049638,\n",
       "        0.00514588, 0.0075182 , 0.01293874, 0.00603662, 0.00866694,\n",
       "        0.01371074, 0.00828176, 0.01133332, 0.01256366, 0.00590796,\n",
       "        0.01062493, 0.01066875, 0.00614705, 0.00920477, 0.01546636,\n",
       "        0.00484724, 0.01007009, 0.01692781, 0.00684743, 0.00950365,\n",
       "        0.0073792 , 0.00462704, 0.00598335, 0.01425204, 0.00611148,\n",
       "        0.01085849, 0.01237712, 0.0064868 , 0.00880103, 0.01119604,\n",
       "        0.00798526, 0.01178608, 0.00880489, 0.00605736, 0.00834322,\n",
       "        0.01144032, 0.00605426, 0.00907817, 0.01112685, 0.00736737,\n",
       "        0.01084146, 0.01027298, 0.00859113, 0.01095357, 0.01235933,\n",
       "        0.00625439, 0.0093904 , 0.01926169, 0.00600524, 0.00821881,\n",
       "        0.01205478, 0.0111342 , 0.00927181, 0.01542897, 0.00558648,\n",
       "        0.00841494, 0.01177177, 0.00473714, 0.00890017, 0.01104693,\n",
       "        0.0105021 , 0.02135029, 0.00989566, 0.02298231, 0.02728839,\n",
       "        0.02459931, 0.0092185 , 0.00861092, 0.00918078, 0.00663304,\n",
       "        0.01155634, 0.01830163, 0.00707874, 0.00803757, 0.01034274,\n",
       "        0.02417583, 0.01061792, 0.009408  , 0.00793548, 0.00930924,\n",
       "        0.01469164, 0.00701942, 0.00818396, 0.00867105, 0.00705047,\n",
       "        0.00768881, 0.01146111, 0.01209922, 0.00886998, 0.01268306,\n",
       "        0.00704842, 0.01206069, 0.01085243, 0.00716877, 0.00883303,\n",
       "        0.01001415, 0.00711913, 0.00645771, 0.01163554, 0.00572162,\n",
       "        0.00819039, 0.01344719, 0.00532222, 0.01021795, 0.01071172,\n",
       "        0.00615802, 0.00598416, 0.01416101, 0.00642939, 0.01044121,\n",
       "        0.00929942, 0.00579276, 0.009408  , 0.0109746 , 0.01233335,\n",
       "        0.00768671, 0.00950899, 0.00833836, 0.00903039, 0.0107697 ,\n",
       "        0.00941834, 0.00834627, 0.01219587, 0.00518146, 0.00949512,\n",
       "        0.01059771, 0.00600424, 0.01209135, 0.00889602, 0.01008773,\n",
       "        0.00932722, 0.01251259, 0.00643415, 0.01438441, 0.01050177,\n",
       "        0.00719457, 0.0084342 , 0.01100755, 0.00603809, 0.0089294 ,\n",
       "        0.01052008, 0.00762696, 0.00948577, 0.01195297, 0.00574274,\n",
       "        0.01022   , 0.01099238, 0.00584569, 0.01400104, 0.01190753,\n",
       "        0.00527534, 0.01056247, 0.0099474 , 0.00657263, 0.01036611,\n",
       "        0.00961566, 0.00711131, 0.01065683, 0.00769134, 0.011201  ,\n",
       "        0.00886173, 0.00802526, 0.01054091, 0.00793161, 0.01055918,\n",
       "        0.00781803, 0.00719495, 0.01028824, 0.00940495, 0.0081861 ,\n",
       "        0.01197538, 0.00615125, 0.01010065, 0.01066837, 0.00577226,\n",
       "        0.00807829, 0.01226296, 0.00710459, 0.0094039 , 0.01117463,\n",
       "        0.00864439, 0.00779061, 0.01360822, 0.00974169, 0.00899715,\n",
       "        0.01285272, 0.0066421 , 0.00939441, 0.01097121, 0.00633883,\n",
       "        0.00949121, 0.01451054, 0.00835433, 0.00984573, 0.01139874,\n",
       "        0.00630932, 0.00883684, 0.00989037, 0.01090555, 0.01199961,\n",
       "        0.00814061, 0.00595503, 0.01005979, 0.00894308, 0.01261125,\n",
       "        0.00729117, 0.00899386, 0.00740223, 0.0077085 , 0.00803723,\n",
       "        0.00969257, 0.00699754, 0.0119256 , 0.00593357, 0.00769939,\n",
       "        0.01099486, 0.00640979, 0.01021562, 0.01043639, 0.00493379,\n",
       "        0.00676627, 0.00994463, 0.00569105, 0.01027684, 0.01104517,\n",
       "        0.0076273 , 0.00753641, 0.01652374, 0.00593247, 0.00750756,\n",
       "        0.01073079, 0.00641379, 0.01099377, 0.01498413, 0.00642118,\n",
       "        0.0088841 , 0.01077542, 0.01033273, 0.00858078, 0.01198063]),\n",
       " 'std_score_time': array([0.00554277, 0.00377018, 0.00274849, 0.00510004, 0.00155372,\n",
       "        0.00224942, 0.00605998, 0.0036067 , 0.00674352, 0.00965134,\n",
       "        0.00041873, 0.00323745, 0.00332933, 0.00304452, 0.00261269,\n",
       "        0.00157796, 0.00628641, 0.00463396, 0.0076982 , 0.00340077,\n",
       "        0.00323742, 0.00081076, 0.00072024, 0.00637261, 0.00243606,\n",
       "        0.00454849, 0.00186161, 0.00487968, 0.0034616 , 0.00328971,\n",
       "        0.00226734, 0.00208593, 0.00098739, 0.00054223, 0.00316366,\n",
       "        0.00440531, 0.00365705, 0.00337923, 0.00356114, 0.0015479 ,\n",
       "        0.00498895, 0.0032231 , 0.00237109, 0.00106791, 0.00139171,\n",
       "        0.00309681, 0.00213836, 0.00435878, 0.00106446, 0.00281217,\n",
       "        0.00238197, 0.00341369, 0.00121988, 0.00235091, 0.00218858,\n",
       "        0.00176433, 0.00266937, 0.00237842, 0.00418735, 0.00368725,\n",
       "        0.00436485, 0.00063152, 0.00177169, 0.00411502, 0.00103163,\n",
       "        0.00122389, 0.00177528, 0.00577034, 0.00079282, 0.00153289,\n",
       "        0.00145552, 0.00172492, 0.00013121, 0.0007043 , 0.00304734,\n",
       "        0.00055025, 0.00218401, 0.00265263, 0.00171631, 0.00660028,\n",
       "        0.00488735, 0.00453614, 0.00093696, 0.00126702, 0.00253083,\n",
       "        0.00126697, 0.00087626, 0.00278736, 0.00822635, 0.00582905,\n",
       "        0.00275478, 0.00045633, 0.00771734, 0.00258157, 0.00232769,\n",
       "        0.00750212, 0.00378821, 0.00272909, 0.0046567 , 0.00479165,\n",
       "        0.00181308, 0.00236529, 0.00182501, 0.0007296 , 0.00390735,\n",
       "        0.00441832, 0.00195908, 0.0029119 , 0.00260933, 0.00254316,\n",
       "        0.00236504, 0.00249504, 0.00168903, 0.00223634, 0.00388678,\n",
       "        0.00247764, 0.00368722, 0.00140022, 0.00222506, 0.00420244,\n",
       "        0.00175523, 0.00189068, 0.00694756, 0.00251138, 0.00248873,\n",
       "        0.00399733, 0.00331708, 0.00255262, 0.00191102, 0.00091138,\n",
       "        0.00380904, 0.00305026, 0.00277794, 0.0026002 , 0.00209238,\n",
       "        0.00171571, 0.00213353, 0.00410983, 0.00199741, 0.00263909,\n",
       "        0.00175632, 0.00160173, 0.0019331 , 0.00145961, 0.00124096,\n",
       "        0.00209025, 0.00441014, 0.00345367, 0.0053171 , 0.00116145,\n",
       "        0.00349059, 0.00812135, 0.00631659, 0.00393513, 0.00158936,\n",
       "        0.00511392, 0.00159014, 0.00288272, 0.00312751, 0.00679485,\n",
       "        0.00086353, 0.00231408, 0.00155491, 0.00117813, 0.00402816,\n",
       "        0.0024097 , 0.00297879, 0.00531217, 0.00238572, 0.00423408,\n",
       "        0.00755342, 0.00043242, 0.00334896, 0.00148601, 0.00208744,\n",
       "        0.00202759, 0.00165486, 0.0031933 , 0.00260168, 0.00613575,\n",
       "        0.00051289, 0.00361894, 0.00098693, 0.0010501 , 0.00319561,\n",
       "        0.00529537, 0.00486076, 0.00416111, 0.00332759, 0.00322877,\n",
       "        0.00171856, 0.00420419, 0.00429053, 0.00355155, 0.0041792 ,\n",
       "        0.00270194, 0.00166792, 0.00208653, 0.00162017, 0.00574314,\n",
       "        0.00102852, 0.00240254, 0.00123285, 0.00129424, 0.00248033,\n",
       "        0.00229197, 0.00362907, 0.00120158, 0.00268424, 0.00452112,\n",
       "        0.00220191, 0.00509588, 0.00142493, 0.00111859, 0.00285898,\n",
       "        0.00248096, 0.00257718, 0.0015564 , 0.00147293, 0.00209718,\n",
       "        0.00104001, 0.00331522, 0.00383881, 0.00158784, 0.00448017,\n",
       "        0.00553583, 0.00254958, 0.0023123 , 0.00128393, 0.00103262,\n",
       "        0.00468526, 0.00116761, 0.00260316, 0.00675491, 0.0022497 ,\n",
       "        0.00240868, 0.001837  , 0.00554549, 0.00213435, 0.00124976,\n",
       "        0.00445785, 0.00130983, 0.00108054, 0.00153114, 0.00107335,\n",
       "        0.00607303, 0.0064711 , 0.00300088, 0.00247002, 0.00195898,\n",
       "        0.00437138, 0.00274106, 0.0022427 , 0.00058347, 0.00439612,\n",
       "        0.00069267, 0.00212464, 0.00400988, 0.00389496, 0.0012189 ,\n",
       "        0.00403566, 0.00205545, 0.00258941, 0.00358559, 0.00148564,\n",
       "        0.00066591, 0.00221817, 0.00310286, 0.00071148, 0.00257144,\n",
       "        0.00064234, 0.00099096, 0.00147094, 0.00158567, 0.00412579,\n",
       "        0.00220865, 0.00174258, 0.00638218, 0.00328492, 0.00312331,\n",
       "        0.00092441, 0.00676922, 0.00160081, 0.00220262, 0.00275152,\n",
       "        0.00126775, 0.00127526, 0.0022353 , 0.00227799, 0.00160508,\n",
       "        0.00321202, 0.00069693, 0.00172809, 0.00506321, 0.00068982,\n",
       "        0.00074261, 0.00267618, 0.00354501, 0.00575145, 0.00329644,\n",
       "        0.00345756, 0.00303589, 0.0053636 , 0.00262847, 0.00599233,\n",
       "        0.00519768, 0.00326222, 0.00129039, 0.00082539, 0.00244907,\n",
       "        0.00266027, 0.00328474, 0.00045695, 0.00184024, 0.00266716,\n",
       "        0.00367438, 0.00084043, 0.00182222, 0.00341549, 0.00544217,\n",
       "        0.00065576, 0.00124123, 0.00254627, 0.00177812, 0.00354786,\n",
       "        0.00212214, 0.00207481, 0.00138037, 0.00568824, 0.00317109,\n",
       "        0.00211915, 0.00243431, 0.00407753, 0.00225818, 0.00206298,\n",
       "        0.00315288, 0.00472587, 0.0021703 , 0.00431276, 0.00056727,\n",
       "        0.00442445, 0.0031955 , 0.00223065, 0.00195813, 0.00413736,\n",
       "        0.00463388, 0.00148871, 0.00250545, 0.00092662, 0.00224937,\n",
       "        0.00189796, 0.00191376, 0.00092017, 0.00145857, 0.00085805,\n",
       "        0.00562074, 0.00356453, 0.00074342, 0.002049  , 0.00333339,\n",
       "        0.00081492, 0.00198194, 0.00206083, 0.00226394, 0.00147487,\n",
       "        0.00134387, 0.00330585, 0.00265036, 0.00274429, 0.00105024,\n",
       "        0.00287153, 0.00288895, 0.00251903, 0.00191063, 0.00224937,\n",
       "        0.00302629, 0.00277868, 0.00798443, 0.00214093, 0.00599898,\n",
       "        0.00505148, 0.00244838, 0.00332252, 0.00218512, 0.0060956 ,\n",
       "        0.00199863, 0.00289498, 0.00595227, 0.00362782, 0.00510412,\n",
       "        0.00401125, 0.0024461 , 0.00188574, 0.0017018 , 0.00292931,\n",
       "        0.00855574, 0.00362413, 0.00169584, 0.01261913, 0.01488666,\n",
       "        0.00768009, 0.0034365 , 0.0043293 , 0.00506479, 0.00823408,\n",
       "        0.00347821, 0.00574452, 0.01670704, 0.00646234, 0.00436307,\n",
       "        0.0033284 , 0.00350063, 0.00188749, 0.01537167, 0.02591668,\n",
       "        0.00171997, 0.00937516, 0.00334156, 0.0072513 , 0.00231313,\n",
       "        0.00251119, 0.00058941, 0.00418691, 0.0031466 , 0.00233696,\n",
       "        0.00762262, 0.00548514, 0.00304474, 0.00625318, 0.00599712,\n",
       "        0.0032298 , 0.00526413, 0.00514223, 0.00211186, 0.00185116,\n",
       "        0.00076549, 0.00110091, 0.00501195, 0.00118027, 0.0022031 ,\n",
       "        0.00343425, 0.00276682, 0.00316358, 0.00336821, 0.0021314 ,\n",
       "        0.0020023 , 0.00271479, 0.00136932, 0.00409205, 0.00679636,\n",
       "        0.00103856, 0.00692033, 0.00887832, 0.00105316, 0.00302914,\n",
       "        0.00063051, 0.00045624, 0.00098315, 0.00490686, 0.00092508,\n",
       "        0.00215314, 0.00491209, 0.00198141, 0.00395075, 0.0038594 ,\n",
       "        0.00390791, 0.00684849, 0.00113394, 0.00181298, 0.00192263,\n",
       "        0.00276804, 0.00152323, 0.003721  , 0.00262597, 0.00158727,\n",
       "        0.0068558 , 0.00259352, 0.00521714, 0.00481286, 0.00186305,\n",
       "        0.00201725, 0.00411183, 0.00581885, 0.00255825, 0.00272725,\n",
       "        0.00360048, 0.00699582, 0.0037155 , 0.00538425, 0.0009797 ,\n",
       "        0.00277826, 0.00233341, 0.00069969, 0.00183429, 0.00245542,\n",
       "        0.00678891, 0.03191704, 0.00208804, 0.01233133, 0.03046291,\n",
       "        0.02268396, 0.00540659, 0.0049129 , 0.00126222, 0.00214733,\n",
       "        0.00665611, 0.00623594, 0.00314367, 0.00295878, 0.0025335 ,\n",
       "        0.01655885, 0.00322186, 0.00220536, 0.00224127, 0.00113683,\n",
       "        0.00796196, 0.00148751, 0.00136704, 0.0006675 , 0.00170144,\n",
       "        0.0015536 , 0.00322473, 0.00394992, 0.00255644, 0.00321721,\n",
       "        0.00279484, 0.00173537, 0.00275392, 0.00509188, 0.00478809,\n",
       "        0.00145487, 0.00231762, 0.0003369 , 0.00509909, 0.0014254 ,\n",
       "        0.00160991, 0.00461803, 0.00107201, 0.00550239, 0.00139503,\n",
       "        0.00155305, 0.00137415, 0.0066611 , 0.0024941 , 0.00359617,\n",
       "        0.00293564, 0.00167263, 0.00266656, 0.00323773, 0.00799024,\n",
       "        0.0013615 , 0.00188611, 0.0024446 , 0.00370265, 0.00446199,\n",
       "        0.00448124, 0.00283972, 0.00452137, 0.00062905, 0.00277397,\n",
       "        0.00186298, 0.00128743, 0.00943265, 0.00111755, 0.00548979,\n",
       "        0.00137883, 0.0036631 , 0.00281355, 0.00398502, 0.00084218,\n",
       "        0.00366499, 0.00141331, 0.00290703, 0.00112878, 0.00293687,\n",
       "        0.00181679, 0.00218188, 0.00295923, 0.00258182, 0.00106109,\n",
       "        0.00356704, 0.00143808, 0.00233222, 0.00418969, 0.00147057,\n",
       "        0.0014346 , 0.00552144, 0.00372842, 0.00282915, 0.00489708,\n",
       "        0.00352259, 0.00319039, 0.00441336, 0.00108184, 0.00586459,\n",
       "        0.00157684, 0.00112033, 0.00527793, 0.00263728, 0.00428031,\n",
       "        0.00468631, 0.00180061, 0.00227023, 0.0044162 , 0.00185109,\n",
       "        0.00310916, 0.00065381, 0.00278926, 0.0023413 , 0.00138433,\n",
       "        0.00250065, 0.00457254, 0.00208085, 0.00286694, 0.00281422,\n",
       "        0.0036129 , 0.00084548, 0.00138899, 0.00493375, 0.00379797,\n",
       "        0.0044168 , 0.00190716, 0.00530484, 0.00167669, 0.00268904,\n",
       "        0.00204264, 0.00481509, 0.00342241, 0.00335803, 0.00168706,\n",
       "        0.00170293, 0.00420912, 0.0040611 , 0.00516944, 0.00563225,\n",
       "        0.00218869, 0.00245487, 0.00337968, 0.0020441 , 0.01385939,\n",
       "        0.00132033, 0.0012965 , 0.00467293, 0.00206477, 0.00124771,\n",
       "        0.00435629, 0.00112267, 0.00239586, 0.00189515, 0.00276593,\n",
       "        0.00327866, 0.00235668, 0.00360589, 0.00264999, 0.001841  ,\n",
       "        0.00077142, 0.00282737, 0.00101517, 0.00270565, 0.0021539 ,\n",
       "        0.00257848, 0.00117022, 0.00451616, 0.00073226, 0.00159429,\n",
       "        0.00253781, 0.00173088, 0.00384435, 0.00254922, 0.00106522,\n",
       "        0.00250128, 0.0021015 , 0.00340451, 0.00244114, 0.00307242]),\n",
       " 'param_colsample_bytree': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45, 0.45,\n",
       "                    0.45, 0.45, 0.45, 0.45, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "                    0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07, 0.07,\n",
       "                    0.07, 0.07, 0.07, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.05, 0.05, 0.05, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "                    0.03, 0.03, 0.03, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3,\n",
       "                    1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500, 100, 300, 500, 100, 300, 500, 100,\n",
       "                    300, 500, 100, 300, 500, 100, 300, 500, 100, 300, 500,\n",
       "                    100, 300, 500, 100, 300, 500, 100, 300, 500, 100, 300,\n",
       "                    500, 100, 300, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.5,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.45,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.07,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.05,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.03,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 6,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'colsample_bytree': 0.4,\n",
       "   'learning_rate': 0.01,\n",
       "   'max_depth': 7,\n",
       "   'min_child_weight': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.85436893, 0.8627451 , 0.85148515, 0.87378641,\n",
       "        0.85714286, 0.8627451 , 0.85714286, 0.86538462, 0.83168317,\n",
       "        0.84615385, 0.80769231, 0.85148515, 0.83018868, 0.83809524,\n",
       "        0.84615385, 0.85714286, 0.84313725, 0.8627451 , 0.84313725,\n",
       "        0.83809524, 0.87128713, 0.85436893, 0.83495146, 0.88235294,\n",
       "        0.8627451 , 0.83495146, 0.84615385, 0.83495146, 0.82242991,\n",
       "        0.8627451 , 0.8627451 , 0.83495146, 0.86538462, 0.85436893,\n",
       "        0.84313725, 0.85436893, 0.81904762, 0.8       , 0.85148515,\n",
       "        0.85436893, 0.83809524, 0.87378641, 0.85148515, 0.83495146,\n",
       "        0.82828283, 0.85436893, 0.83809524, 0.84848485, 0.87378641,\n",
       "        0.86538462, 0.8627451 , 0.86538462, 0.86538462, 0.83168317,\n",
       "        0.85436893, 0.84615385, 0.84313725, 0.8627451 , 0.83809524,\n",
       "        0.8627451 , 0.86538462, 0.85436893, 0.86      , 0.8627451 ,\n",
       "        0.86538462, 0.85148515, 0.84313725, 0.85436893, 0.8627451 ,\n",
       "        0.87378641, 0.87128713, 0.8627451 , 0.85148515, 0.83495146,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.8627451 , 0.85436893,\n",
       "        0.84313725, 0.8627451 , 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.85436893, 0.85436893, 0.85436893, 0.85148515,\n",
       "        0.82      , 0.85148515, 0.85436893, 0.83673469, 0.8627451 ,\n",
       "        0.86538462, 0.84848485, 0.85436893, 0.86538462, 0.81188119,\n",
       "        0.86538462, 0.86538462, 0.81188119, 0.85714286, 0.85714286,\n",
       "        0.81188119, 0.87378641, 0.85714286, 0.84848485, 0.8627451 ,\n",
       "        0.8627451 , 0.84      , 0.85436893, 0.81553398, 0.83495146,\n",
       "        0.86538462, 0.8490566 , 0.84      , 0.8627451 , 0.85148515,\n",
       "        0.84313725, 0.86538462, 0.85436893, 0.83495146, 0.85714286,\n",
       "        0.85714286, 0.85148515, 0.85148515, 0.84615385, 0.85436893,\n",
       "        0.84615385, 0.85436893, 0.83495146, 0.86792453, 0.85148515,\n",
       "        0.82828283, 0.84      , 0.83495146, 0.84848485, 0.85148515,\n",
       "        0.8627451 , 0.83673469, 0.85436893, 0.84615385, 0.82474227,\n",
       "        0.83168317, 0.8627451 , 0.82828283, 0.84313725, 0.8627451 ,\n",
       "        0.84      , 0.84313725, 0.87378641, 0.83673469, 0.86      ,\n",
       "        0.85436893, 0.83673469, 0.86      , 0.8627451 , 0.84      ,\n",
       "        0.8627451 , 0.86538462, 0.82828283, 0.8627451 , 0.8627451 ,\n",
       "        0.84      , 0.87378641, 0.87378641, 0.82828283, 0.85148515,\n",
       "        0.86538462, 0.83168317, 0.85148515, 0.85148515, 0.84      ,\n",
       "        0.87378641, 0.8627451 , 0.84      , 0.85436893, 0.85436893,\n",
       "        0.82105263, 0.83673469, 0.84848485, 0.82105263, 0.83673469,\n",
       "        0.84848485, 0.80851064, 0.83673469, 0.82828283, 0.8       ,\n",
       "        0.83673469, 0.84848485, 0.82105263, 0.83673469, 0.84848485,\n",
       "        0.8       , 0.83673469, 0.84      , 0.80851064, 0.83673469,\n",
       "        0.84848485, 0.82105263, 0.84848485, 0.84848485, 0.78723404,\n",
       "        0.84313725, 0.84      , 0.8       , 0.83673469, 0.83673469,\n",
       "        0.85416667, 0.82828283, 0.84      , 0.83673469, 0.84313725,\n",
       "        0.84313725, 0.83333333, 0.82828283, 0.82828283, 0.84536082,\n",
       "        0.82828283, 0.84      , 0.82828283, 0.84313725, 0.84313725,\n",
       "        0.82352941, 0.85148515, 0.84      , 0.86      , 0.87378641,\n",
       "        0.85714286, 0.85436893, 0.85714286, 0.85714286, 0.81553398,\n",
       "        0.83018868, 0.81904762, 0.84615385, 0.85714286, 0.85714286,\n",
       "        0.83495146, 0.86538462, 0.84615385, 0.83495146, 0.81553398,\n",
       "        0.80392157, 0.85436893, 0.84313725, 0.81904762, 0.85436893,\n",
       "        0.85436893, 0.83495146, 0.82692308, 0.83809524, 0.7961165 ,\n",
       "        0.8627451 , 0.85436893, 0.85436893, 0.84      , 0.86538462,\n",
       "        0.84615385, 0.85148515, 0.83168317, 0.80769231, 0.8627451 ,\n",
       "        0.8627451 , 0.87128713, 0.85436893, 0.8411215 , 0.82692308,\n",
       "        0.83168317, 0.83495146, 0.83809524, 0.84      , 0.8627451 ,\n",
       "        0.86538462, 0.84313725, 0.87378641, 0.86538462, 0.82352941,\n",
       "        0.85436893, 0.83809524, 0.83168317, 0.84615385, 0.85436893,\n",
       "        0.82352941, 0.85436893, 0.85436893, 0.82352941, 0.83495146,\n",
       "        0.83495146, 0.83168317, 0.84313725, 0.84313725, 0.82352941,\n",
       "        0.84615385, 0.86538462, 0.84615385, 0.83495146, 0.82692308,\n",
       "        0.85436893, 0.84313725, 0.84313725, 0.83495146, 0.85714286,\n",
       "        0.83809524, 0.84      , 0.84313725, 0.81553398, 0.8627451 ,\n",
       "        0.85148515, 0.8627451 , 0.85436893, 0.85714286, 0.85436893,\n",
       "        0.80392157, 0.82352941, 0.83809524, 0.85148515, 0.85148515,\n",
       "        0.8627451 , 0.84313725, 0.84313725, 0.86538462, 0.82352941,\n",
       "        0.83495146, 0.85436893, 0.84      , 0.85436893, 0.84615385,\n",
       "        0.82352941, 0.84313725, 0.84615385, 0.80769231, 0.84313725,\n",
       "        0.85148515, 0.82352941, 0.85436893, 0.84615385, 0.81553398,\n",
       "        0.83495146, 0.83809524, 0.80392157, 0.84313725, 0.84313725,\n",
       "        0.83168317, 0.84313725, 0.83495146, 0.81553398, 0.85436893,\n",
       "        0.85714286, 0.83168317, 0.84313725, 0.84313725, 0.8627451 ,\n",
       "        0.85148515, 0.84      , 0.82352941, 0.86538462, 0.84615385,\n",
       "        0.82      , 0.82352941, 0.83495146, 0.84536082, 0.84313725,\n",
       "        0.85148515, 0.81632653, 0.84615385, 0.85436893, 0.8       ,\n",
       "        0.83168317, 0.85436893, 0.80808081, 0.84313725, 0.85436893,\n",
       "        0.76767677, 0.85148515, 0.85148515, 0.8       , 0.83168317,\n",
       "        0.8627451 , 0.81632653, 0.84313725, 0.8627451 , 0.7755102 ,\n",
       "        0.82352941, 0.8627451 , 0.82828283, 0.83168317, 0.84313725,\n",
       "        0.82474227, 0.85148515, 0.85436893, 0.79591837, 0.84      ,\n",
       "        0.8627451 , 0.81632653, 0.85148515, 0.85148515, 0.82474227,\n",
       "        0.85148515, 0.8627451 , 0.79591837, 0.84313725, 0.86538462,\n",
       "        0.75555556, 0.8125    , 0.81188119, 0.76404494, 0.82105263,\n",
       "        0.84      , 0.74157303, 0.81632653, 0.84615385, 0.76595745,\n",
       "        0.79591837, 0.84      , 0.77777778, 0.80412371, 0.84848485,\n",
       "        0.71111111, 0.7628866 , 0.82352941, 0.76595745, 0.82      ,\n",
       "        0.83168317, 0.80434783, 0.81632653, 0.84      , 0.75268817,\n",
       "        0.79591837, 0.83168317, 0.77894737, 0.80808081, 0.82      ,\n",
       "        0.78723404, 0.81632653, 0.84      , 0.76086957, 0.78350515,\n",
       "        0.8       , 0.75      , 0.82828283, 0.82      , 0.7826087 ,\n",
       "        0.82474227, 0.82828283, 0.76923077, 0.79591837, 0.83168317]),\n",
       " 'split1_test_score': array([0.70707071, 0.72      , 0.70588235, 0.74747475, 0.72      ,\n",
       "        0.71287129, 0.74226804, 0.71428571, 0.74747475, 0.73469388,\n",
       "        0.7       , 0.70588235, 0.69473684, 0.71287129, 0.73267327,\n",
       "        0.72164948, 0.72916667, 0.76      , 0.72164948, 0.7184466 ,\n",
       "        0.69230769, 0.70103093, 0.72      , 0.74285714, 0.72164948,\n",
       "        0.74226804, 0.76470588, 0.72164948, 0.7254902 , 0.7047619 ,\n",
       "        0.6875    , 0.72      , 0.73786408, 0.70707071, 0.73267327,\n",
       "        0.75728155, 0.71428571, 0.7047619 , 0.69811321, 0.70103093,\n",
       "        0.74509804, 0.73786408, 0.71428571, 0.76      , 0.78      ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.70707071, 0.72      , 0.70588235, 0.74747475, 0.72      ,\n",
       "        0.71287129, 0.74226804, 0.71428571, 0.74747475, 0.73469388,\n",
       "        0.7       , 0.70588235, 0.69473684, 0.71287129, 0.73267327,\n",
       "        0.72164948, 0.72916667, 0.76      , 0.72164948, 0.7184466 ,\n",
       "        0.69230769, 0.70103093, 0.72      , 0.74285714, 0.72164948,\n",
       "        0.74226804, 0.76470588, 0.72164948, 0.7254902 , 0.7047619 ,\n",
       "        0.6875    , 0.72      , 0.73786408, 0.70707071, 0.73267327,\n",
       "        0.75728155, 0.71428571, 0.7047619 , 0.69811321, 0.70103093,\n",
       "        0.74509804, 0.73786408, 0.71428571, 0.76      , 0.78      ,\n",
       "        0.74509804, 0.72      , 0.7       , 0.74      , 0.74      ,\n",
       "        0.74509804, 0.74226804, 0.74      , 0.73267327, 0.70833333,\n",
       "        0.71428571, 0.73786408, 0.69387755, 0.7       , 0.7184466 ,\n",
       "        0.72916667, 0.72164948, 0.74226804, 0.68041237, 0.7       ,\n",
       "        0.7184466 , 0.66666667, 0.71428571, 0.75      , 0.72164948,\n",
       "        0.72727273, 0.74747475, 0.69387755, 0.7184466 , 0.71153846,\n",
       "        0.70103093, 0.70707071, 0.74285714, 0.72164948, 0.72      ,\n",
       "        0.76470588, 0.67368421, 0.7184466 , 0.71153846, 0.71287129,\n",
       "        0.72      , 0.74      , 0.70103093, 0.74747475, 0.76767677,\n",
       "        0.7       , 0.72      , 0.73267327, 0.71428571, 0.71428571,\n",
       "        0.74      , 0.72727273, 0.74      , 0.72727273, 0.70103093,\n",
       "        0.71428571, 0.72      , 0.71428571, 0.70103093, 0.71428571,\n",
       "        0.71428571, 0.71428571, 0.70707071, 0.68686869, 0.71428571,\n",
       "        0.71287129, 0.68041237, 0.70103093, 0.68686869, 0.71428571,\n",
       "        0.70707071, 0.71428571, 0.70103093, 0.72      , 0.7254902 ,\n",
       "        0.65979381, 0.71287129, 0.70588235, 0.70103093, 0.70707071,\n",
       "        0.74      , 0.6875    , 0.72727273, 0.7254902 , 0.68041237,\n",
       "        0.73267327, 0.7184466 , 0.70103093, 0.72      , 0.75247525,\n",
       "        0.71428571, 0.72727273, 0.74      , 0.67368421, 0.72      ,\n",
       "        0.71428571, 0.70833333, 0.72727273, 0.71428571, 0.6875    ,\n",
       "        0.70103093, 0.72727273, 0.6875    , 0.71428571, 0.72164948,\n",
       "        0.70833333, 0.71428571, 0.71428571, 0.70103093, 0.68041237,\n",
       "        0.70103093, 0.68686869, 0.70707071, 0.68686869, 0.70707071,\n",
       "        0.71428571, 0.69387755, 0.6875    , 0.69387755, 0.72      ,\n",
       "        0.65979381, 0.68041237, 0.70707071, 0.71428571, 0.70707071,\n",
       "        0.69387755, 0.70103093, 0.70103093, 0.74      , 0.67346939,\n",
       "        0.69387755, 0.71428571, 0.71428571, 0.70707071, 0.70103093,\n",
       "        0.68085106, 0.67346939, 0.7       , 0.66666667, 0.66666667,\n",
       "        0.70707071, 0.66666667, 0.68686869, 0.69387755, 0.66666667,\n",
       "        0.6875    , 0.68686869, 0.65979381, 0.66666667, 0.70707071,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.67346939, 0.68686869,\n",
       "        0.68686869, 0.67346939, 0.69387755, 0.70707071, 0.69387755,\n",
       "        0.68      , 0.70707071, 0.6875    , 0.7       , 0.7       ,\n",
       "        0.67346939, 0.66666667, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.69387755, 0.68041237, 0.68686869, 0.68686869, 0.66666667,\n",
       "        0.68      , 0.70707071, 0.70707071, 0.68      , 0.7       ,\n",
       "        0.73267327, 0.7254902 , 0.68      , 0.72      , 0.71428571,\n",
       "        0.71287129, 0.70707071, 0.71428571, 0.71428571, 0.70707071,\n",
       "        0.72      , 0.69902913, 0.70707071, 0.69387755, 0.72727273,\n",
       "        0.70707071, 0.70103093, 0.72164948, 0.69387755, 0.69387755,\n",
       "        0.7184466 , 0.71428571, 0.72      , 0.73076923, 0.70833333,\n",
       "        0.72164948, 0.74747475, 0.68686869, 0.70588235, 0.71153846,\n",
       "        0.66666667, 0.7       , 0.73076923, 0.71428571, 0.72727273,\n",
       "        0.73267327, 0.71287129, 0.71153846, 0.7047619 , 0.68041237,\n",
       "        0.71287129, 0.72380952, 0.71428571, 0.72727273, 0.74      ,\n",
       "        0.72      , 0.73267327, 0.74      , 0.7       , 0.72727273,\n",
       "        0.72      , 0.70103093, 0.70707071, 0.71428571, 0.69387755,\n",
       "        0.72727273, 0.69306931, 0.7       , 0.72164948, 0.71428571,\n",
       "        0.69387755, 0.68686869, 0.71428571, 0.67326733, 0.7       ,\n",
       "        0.72380952, 0.7       , 0.70833333, 0.70103093, 0.69387755,\n",
       "        0.7       , 0.70707071, 0.69306931, 0.72      , 0.70588235,\n",
       "        0.70707071, 0.72164948, 0.73267327, 0.69387755, 0.70707071,\n",
       "        0.74      , 0.67326733, 0.73786408, 0.71153846, 0.70707071,\n",
       "        0.70707071, 0.7254902 , 0.69387755, 0.69387755, 0.72      ,\n",
       "        0.66666667, 0.72      , 0.74509804, 0.68      , 0.72      ,\n",
       "        0.71428571, 0.69387755, 0.72      , 0.72727273, 0.67346939,\n",
       "        0.71428571, 0.72      , 0.67346939, 0.70707071, 0.71428571,\n",
       "        0.70707071, 0.68686869, 0.69387755, 0.67326733, 0.69387755,\n",
       "        0.68686869, 0.69306931, 0.72      , 0.71428571, 0.69387755,\n",
       "        0.69387755, 0.69387755, 0.67326733, 0.70707071, 0.72      ,\n",
       "        0.68686869, 0.71428571, 0.70707071, 0.69387755, 0.68686869,\n",
       "        0.70707071, 0.66      , 0.72      , 0.71153846, 0.7       ,\n",
       "        0.69387755, 0.72      , 0.69387755, 0.7       , 0.70707071,\n",
       "        0.66666667, 0.73267327, 0.72      , 0.65306122, 0.70707071,\n",
       "        0.70707071, 0.68686869, 0.72      , 0.72      , 0.65306122,\n",
       "        0.68686869, 0.71428571, 0.65306122, 0.70707071, 0.70707071,\n",
       "        0.66666667, 0.70707071, 0.70707071, 0.65306122, 0.67346939,\n",
       "        0.70707071, 0.65306122, 0.7       , 0.69387755, 0.67346939,\n",
       "        0.68686869, 0.68686869, 0.66666667, 0.69306931, 0.70707071,\n",
       "        0.66666667, 0.68686869, 0.70707071, 0.68686869, 0.68686869,\n",
       "        0.7       , 0.66666667, 0.7       , 0.71287129, 0.65306122,\n",
       "        0.68686869, 0.7       , 0.68686869, 0.69387755, 0.7       ,\n",
       "        0.60215054, 0.66666667, 0.66666667, 0.60215054, 0.66666667,\n",
       "        0.68686869, 0.60869565, 0.68      , 0.68686869, 0.63157895,\n",
       "        0.65306122, 0.66666667, 0.61052632, 0.65306122, 0.68      ,\n",
       "        0.63157895, 0.66666667, 0.70707071, 0.625     , 0.65306122,\n",
       "        0.68      , 0.63917526, 0.65306122, 0.69306931, 0.65979381,\n",
       "        0.68041237, 0.70707071, 0.63917526, 0.66      , 0.68627451,\n",
       "        0.63917526, 0.66666667, 0.68686869, 0.65979381, 0.69387755,\n",
       "        0.70707071, 0.63917526, 0.66      , 0.68627451, 0.63157895,\n",
       "        0.67346939, 0.68686869, 0.64583333, 0.69387755, 0.69387755]),\n",
       " 'split2_test_score': array([0.78095238, 0.75      , 0.74285714, 0.75      , 0.74285714,\n",
       "        0.74285714, 0.73786408, 0.76470588, 0.76190476, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.76190476, 0.75      , 0.75      ,\n",
       "        0.73786408, 0.76190476, 0.75471698, 0.74285714, 0.73584906,\n",
       "        0.74285714, 0.76190476, 0.75      , 0.74285714, 0.75      ,\n",
       "        0.74285714, 0.73394495, 0.72380952, 0.73584906, 0.73584906,\n",
       "        0.76923077, 0.75      , 0.74285714, 0.73786408, 0.75471698,\n",
       "        0.72727273, 0.73584906, 0.73584906, 0.72897196, 0.75471698,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75471698, 0.73394495,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.78095238, 0.75      , 0.74285714, 0.75      , 0.74285714,\n",
       "        0.74285714, 0.73786408, 0.76470588, 0.76190476, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.76190476, 0.75      , 0.75      ,\n",
       "        0.73786408, 0.76190476, 0.75471698, 0.74285714, 0.73584906,\n",
       "        0.74285714, 0.76190476, 0.75      , 0.74285714, 0.75      ,\n",
       "        0.74285714, 0.73394495, 0.72380952, 0.73584906, 0.73584906,\n",
       "        0.76923077, 0.75      , 0.74285714, 0.73786408, 0.75471698,\n",
       "        0.72727273, 0.73584906, 0.73584906, 0.72897196, 0.75471698,\n",
       "        0.74285714, 0.74285714, 0.75728155, 0.75471698, 0.73394495,\n",
       "        0.74509804, 0.74285714, 0.73584906, 0.75728155, 0.75      ,\n",
       "        0.75      , 0.75728155, 0.74285714, 0.75728155, 0.74285714,\n",
       "        0.74766355, 0.73584906, 0.75      , 0.74285714, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.75471698, 0.73076923, 0.72380952,\n",
       "        0.73584906, 0.74285714, 0.74285714, 0.75      , 0.73786408,\n",
       "        0.75728155, 0.74285714, 0.73076923, 0.73584906, 0.73584906,\n",
       "        0.75      , 0.74285714, 0.74285714, 0.75      , 0.74285714,\n",
       "        0.75471698, 0.73584906, 0.72222222, 0.72222222, 0.74285714,\n",
       "        0.74285714, 0.73584906, 0.73076923, 0.74285714, 0.73394495,\n",
       "        0.76470588, 0.72380952, 0.75471698, 0.76470588, 0.73584906,\n",
       "        0.75      , 0.76470588, 0.73584906, 0.74285714, 0.73786408,\n",
       "        0.72380952, 0.72380952, 0.75728155, 0.75      , 0.74285714,\n",
       "        0.76190476, 0.75      , 0.75      , 0.71153846, 0.72897196,\n",
       "        0.73584906, 0.74285714, 0.76190476, 0.74285714, 0.74285714,\n",
       "        0.75      , 0.75      , 0.71698113, 0.72222222, 0.72897196,\n",
       "        0.73076923, 0.73584906, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.71698113, 0.72222222, 0.72222222, 0.73076923,\n",
       "        0.73584906, 0.75      , 0.74285714, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75728155, 0.75925926, 0.75247525, 0.75728155,\n",
       "        0.74285714, 0.74      , 0.75728155, 0.75      , 0.74509804,\n",
       "        0.72380952, 0.75471698, 0.74509804, 0.75      , 0.73584906,\n",
       "        0.73076923, 0.75      , 0.73786408, 0.73076923, 0.74766355,\n",
       "        0.72897196, 0.73076923, 0.74285714, 0.75      , 0.72380952,\n",
       "        0.75      , 0.73786408, 0.72380952, 0.71028037, 0.74074074,\n",
       "        0.73076923, 0.74285714, 0.74285714, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.71698113, 0.71028037, 0.72222222, 0.72380952,\n",
       "        0.73076923, 0.72897196, 0.72380952, 0.73786408, 0.73584906,\n",
       "        0.72727273, 0.74      , 0.75247525, 0.72727273, 0.74      ,\n",
       "        0.75247525, 0.72727273, 0.75247525, 0.75728155, 0.73267327,\n",
       "        0.74509804, 0.75247525, 0.74      , 0.75247525, 0.74509804,\n",
       "        0.74      , 0.75728155, 0.75      , 0.75728155, 0.73076923,\n",
       "        0.73076923, 0.75728155, 0.73786408, 0.73076923, 0.73786408,\n",
       "        0.74285714, 0.75      , 0.73267327, 0.73076923, 0.71698113,\n",
       "        0.75247525, 0.73076923, 0.73076923, 0.72380952, 0.74285714,\n",
       "        0.74285714, 0.73267327, 0.71698113, 0.71698113, 0.75247525,\n",
       "        0.72380952, 0.73076923, 0.72380952, 0.74285714, 0.74285714,\n",
       "        0.76190476, 0.74074074, 0.72897196, 0.75471698, 0.74285714,\n",
       "        0.74285714, 0.75471698, 0.74285714, 0.74285714, 0.74285714,\n",
       "        0.72897196, 0.72222222, 0.73584906, 0.75      , 0.73584906,\n",
       "        0.76190476, 0.74285714, 0.75471698, 0.74285714, 0.74074074,\n",
       "        0.72897196, 0.76190476, 0.74766355, 0.74285714, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.75471698, 0.73394495, 0.73394495,\n",
       "        0.76190476, 0.74285714, 0.73584906, 0.74285714, 0.73584906,\n",
       "        0.72727273, 0.75471698, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.73584906, 0.73584906, 0.73076923, 0.73584906, 0.72727273,\n",
       "        0.75247525, 0.75471698, 0.72897196, 0.73076923, 0.74074074,\n",
       "        0.75      , 0.75      , 0.73584906, 0.75      , 0.72380952,\n",
       "        0.73786408, 0.74074074, 0.73584906, 0.75471698, 0.74766355,\n",
       "        0.75      , 0.75      , 0.75471698, 0.73786408, 0.73394495,\n",
       "        0.73394495, 0.73584906, 0.76190476, 0.73584906, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.72380952, 0.73394495, 0.72222222,\n",
       "        0.73076923, 0.72897196, 0.74285714, 0.73076923, 0.74285714,\n",
       "        0.75471698, 0.71153846, 0.72897196, 0.72222222, 0.74285714,\n",
       "        0.74074074, 0.73584906, 0.72380952, 0.73584906, 0.75471698,\n",
       "        0.75247525, 0.75925926, 0.75229358, 0.73267327, 0.74766355,\n",
       "        0.73584906, 0.74509804, 0.75      , 0.74285714, 0.75247525,\n",
       "        0.7254902 , 0.73584906, 0.73267327, 0.74285714, 0.75471698,\n",
       "        0.74509804, 0.75      , 0.76190476, 0.73267327, 0.74285714,\n",
       "        0.72897196, 0.7254902 , 0.75471698, 0.74766355, 0.73786408,\n",
       "        0.74285714, 0.74285714, 0.7184466 , 0.74545455, 0.73394495,\n",
       "        0.71698113, 0.75471698, 0.74285714, 0.73786408, 0.73584906,\n",
       "        0.74285714, 0.7047619 , 0.73873874, 0.73394495, 0.73076923,\n",
       "        0.74766355, 0.73584906, 0.73076923, 0.73584906, 0.74285714,\n",
       "        0.74      , 0.76923077, 0.74766355, 0.74      , 0.75471698,\n",
       "        0.75471698, 0.74      , 0.76190476, 0.75471698, 0.74509804,\n",
       "        0.75      , 0.76635514, 0.74509804, 0.75      , 0.74285714,\n",
       "        0.73267327, 0.75      , 0.76190476, 0.74509804, 0.73584906,\n",
       "        0.75925926, 0.74509804, 0.74285714, 0.75471698, 0.73786408,\n",
       "        0.74285714, 0.76190476, 0.73267327, 0.74285714, 0.74545455,\n",
       "        0.74509804, 0.73786408, 0.75471698, 0.73076923, 0.74285714,\n",
       "        0.74285714, 0.74509804, 0.75471698, 0.74545455, 0.73786408,\n",
       "        0.74285714, 0.74766355, 0.73076923, 0.74285714, 0.74285714,\n",
       "        0.70833333, 0.74      , 0.75247525, 0.70833333, 0.73267327,\n",
       "        0.74509804, 0.75510204, 0.74      , 0.76470588, 0.74      ,\n",
       "        0.75728155, 0.75728155, 0.74747475, 0.74509804, 0.74509804,\n",
       "        0.74747475, 0.73786408, 0.75      , 0.76      , 0.75      ,\n",
       "        0.75      , 0.75247525, 0.74509804, 0.73076923, 0.74      ,\n",
       "        0.73786408, 0.74285714, 0.76      , 0.75      , 0.72222222,\n",
       "        0.74      , 0.73786408, 0.73076923, 0.74509804, 0.73786408,\n",
       "        0.75      , 0.74      , 0.74285714, 0.72897196, 0.74      ,\n",
       "        0.73786408, 0.72380952, 0.74509804, 0.73076923, 0.75      ]),\n",
       " 'split3_test_score': array([0.78095238, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.7961165 , 0.78846154, 0.8       , 0.81553398, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.76190476, 0.80769231, 0.77669903, 0.76923077, 0.80769231,\n",
       "        0.80392157, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.8       , 0.76470588, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.77358491, 0.79245283, 0.77669903, 0.77669903, 0.8       ,\n",
       "        0.77669903, 0.76923077, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.78095238, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.7961165 , 0.78846154, 0.8       , 0.81553398, 0.80769231,\n",
       "        0.77358491, 0.79245283, 0.79245283, 0.78846154, 0.77358491,\n",
       "        0.7961165 , 0.80769231, 0.80769231, 0.78846154, 0.78095238,\n",
       "        0.76190476, 0.80769231, 0.77669903, 0.76923077, 0.80769231,\n",
       "        0.80392157, 0.78846154, 0.79245283, 0.78846154, 0.77669903,\n",
       "        0.8       , 0.76470588, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.77358491, 0.79245283, 0.77669903, 0.77669903, 0.8       ,\n",
       "        0.77669903, 0.76923077, 0.80769231, 0.7961165 , 0.78095238,\n",
       "        0.76470588, 0.78095238, 0.78846154, 0.78846154, 0.78846154,\n",
       "        0.78846154, 0.78846154, 0.80769231, 0.80769231, 0.8       ,\n",
       "        0.78095238, 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.77669903, 0.80769231, 0.81904762, 0.80769231, 0.78095238,\n",
       "        0.77358491, 0.81553398, 0.78846154, 0.77669903, 0.7961165 ,\n",
       "        0.81553398, 0.80392157, 0.80769231, 0.78846154, 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.78095238, 0.80769231, 0.7961165 ,\n",
       "        0.81553398, 0.78846154, 0.78095238, 0.77669903, 0.8       ,\n",
       "        0.76923077, 0.77358491, 0.80769231, 0.7961165 , 0.78846154,\n",
       "        0.76470588, 0.78846154, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.77227723, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.78846154, 0.77358491, 0.77669903, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.80769231, 0.80769231, 0.78095238, 0.78095238,\n",
       "        0.77777778, 0.7961165 , 0.78846154, 0.78095238, 0.7961165 ,\n",
       "        0.80769231, 0.7961165 , 0.81553398, 0.78095238, 0.76923077,\n",
       "        0.80769231, 0.8       , 0.78095238, 0.80769231, 0.80769231,\n",
       "        0.7961165 , 0.80769231, 0.78095238, 0.76923077, 0.80769231,\n",
       "        0.79245283, 0.8       , 0.80769231, 0.80769231, 0.7961165 ,\n",
       "        0.77227723, 0.78846154, 0.78095238, 0.76470588, 0.78846154,\n",
       "        0.8       , 0.77227723, 0.78846154, 0.80769231, 0.78431373,\n",
       "        0.8       , 0.78095238, 0.77669903, 0.79245283, 0.8       ,\n",
       "        0.77669903, 0.8       , 0.80769231, 0.80392157, 0.7961165 ,\n",
       "        0.7961165 , 0.78095238, 0.8       , 0.79245283, 0.80392157,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80392157, 0.78095238,\n",
       "        0.80769231, 0.8       , 0.79245283, 0.80392157, 0.80769231,\n",
       "        0.80769231, 0.80769231, 0.78846154, 0.78095238, 0.80769231,\n",
       "        0.79245283, 0.78504673, 0.80392157, 0.80769231, 0.80769231,\n",
       "        0.75510204, 0.76470588, 0.77669903, 0.75510204, 0.76470588,\n",
       "        0.76923077, 0.74      , 0.76470588, 0.76923077, 0.77227723,\n",
       "        0.76470588, 0.78846154, 0.76470588, 0.77669903, 0.76923077,\n",
       "        0.75728155, 0.77669903, 0.76923077, 0.80392157, 0.7961165 ,\n",
       "        0.80392157, 0.80392157, 0.7961165 , 0.7961165 , 0.77669903,\n",
       "        0.78431373, 0.7961165 , 0.80392157, 0.78431373, 0.81553398,\n",
       "        0.79207921, 0.7961165 , 0.80769231, 0.77669903, 0.7961165 ,\n",
       "        0.80769231, 0.80392157, 0.80392157, 0.81553398, 0.79207921,\n",
       "        0.80769231, 0.8       , 0.77669903, 0.7961165 , 0.81553398,\n",
       "        0.81132075, 0.79245283, 0.79245283, 0.8       , 0.78846154,\n",
       "        0.80769231, 0.78846154, 0.80769231, 0.81553398, 0.82242991,\n",
       "        0.8       , 0.79245283, 0.81132075, 0.8       , 0.79245283,\n",
       "        0.80769231, 0.80769231, 0.81553398, 0.80769231, 0.78095238,\n",
       "        0.78095238, 0.82242991, 0.79245283, 0.79245283, 0.81904762,\n",
       "        0.81904762, 0.82352941, 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.81904762, 0.78504673, 0.78504673, 0.81904762, 0.80769231,\n",
       "        0.80769231, 0.81904762, 0.78504673, 0.79245283, 0.81904762,\n",
       "        0.78504673, 0.76635514, 0.81904762, 0.7961165 , 0.80769231,\n",
       "        0.77669903, 0.79245283, 0.79245283, 0.77669903, 0.78095238,\n",
       "        0.78846154, 0.77669903, 0.81132075, 0.7961165 , 0.80769231,\n",
       "        0.8       , 0.78095238, 0.8       , 0.78846154, 0.79245283,\n",
       "        0.7961165 , 0.81904762, 0.82692308, 0.82692308, 0.79245283,\n",
       "        0.79245283, 0.82692308, 0.79245283, 0.78504673, 0.80769231,\n",
       "        0.81904762, 0.81904762, 0.81553398, 0.78846154, 0.77358491,\n",
       "        0.82692308, 0.77358491, 0.78504673, 0.81553398, 0.80769231,\n",
       "        0.81132075, 0.80769231, 0.79245283, 0.77777778, 0.80769231,\n",
       "        0.78504673, 0.78504673, 0.82692308, 0.81132075, 0.8       ,\n",
       "        0.77669903, 0.81132075, 0.78095238, 0.77669903, 0.79245283,\n",
       "        0.78846154, 0.78431373, 0.8       , 0.7961165 , 0.7961165 ,\n",
       "        0.8       , 0.78095238, 0.78431373, 0.8       , 0.78846154,\n",
       "        0.78431373, 0.80769231, 0.80769231, 0.80392157, 0.78095238,\n",
       "        0.78504673, 0.7961165 , 0.79245283, 0.78504673, 0.78846154,\n",
       "        0.8       , 0.81904762, 0.82692308, 0.78846154, 0.77358491,\n",
       "        0.80769231, 0.78846154, 0.78504673, 0.80769231, 0.80769231,\n",
       "        0.8       , 0.80769231, 0.78095238, 0.78095238, 0.80769231,\n",
       "        0.78504673, 0.79245283, 0.81553398, 0.81904762, 0.81132075,\n",
       "        0.78      , 0.76923077, 0.78095238, 0.76470588, 0.78095238,\n",
       "        0.79245283, 0.76470588, 0.78095238, 0.78095238, 0.77669903,\n",
       "        0.80373832, 0.78095238, 0.77669903, 0.79245283, 0.79245283,\n",
       "        0.77669903, 0.79245283, 0.8       , 0.7961165 , 0.79245283,\n",
       "        0.79245283, 0.7961165 , 0.79245283, 0.79245283, 0.77669903,\n",
       "        0.79245283, 0.8       , 0.7961165 , 0.78846154, 0.78095238,\n",
       "        0.77669903, 0.8       , 0.79245283, 0.7961165 , 0.8       ,\n",
       "        0.81132075, 0.78846154, 0.78095238, 0.79245283, 0.78846154,\n",
       "        0.79245283, 0.7962963 , 0.7961165 , 0.81132075, 0.8       ,\n",
       "        0.77419355, 0.78787879, 0.76470588, 0.77419355, 0.78787879,\n",
       "        0.76470588, 0.75789474, 0.78787879, 0.76470588, 0.77083333,\n",
       "        0.78      , 0.77669903, 0.7628866 , 0.78      , 0.77669903,\n",
       "        0.7755102 , 0.78      , 0.77669903, 0.82828283, 0.76470588,\n",
       "        0.78846154, 0.78787879, 0.76470588, 0.78846154, 0.7755102 ,\n",
       "        0.76      , 0.78846154, 0.79166667, 0.77227723, 0.8       ,\n",
       "        0.7628866 , 0.76470588, 0.79245283, 0.7628866 , 0.76      ,\n",
       "        0.8       , 0.8125    , 0.77669903, 0.78846154, 0.7628866 ,\n",
       "        0.76923077, 0.8       , 0.7628866 , 0.78      , 0.80769231]),\n",
       " 'split4_test_score': array([0.7628866 , 0.79591837, 0.78787879, 0.75510204, 0.75789474,\n",
       "        0.78787879, 0.71428571, 0.73469388, 0.7628866 , 0.76767677,\n",
       "        0.80412371, 0.79591837, 0.72164948, 0.7628866 , 0.78787879,\n",
       "        0.70103093, 0.74226804, 0.7628866 , 0.80808081, 0.82474227,\n",
       "        0.81632653, 0.77083333, 0.78350515, 0.76767677, 0.74226804,\n",
       "        0.7628866 , 0.7755102 , 0.80412371, 0.81632653, 0.7628866 ,\n",
       "        0.74226804, 0.78      , 0.7755102 , 0.7755102 , 0.75789474,\n",
       "        0.78787879, 0.80412371, 0.80412371, 0.7628866 , 0.7755102 ,\n",
       "        0.78787879, 0.76767677, 0.7755102 , 0.75      , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.7628866 , 0.79591837, 0.78787879, 0.75510204, 0.75789474,\n",
       "        0.78787879, 0.71428571, 0.73469388, 0.7628866 , 0.76767677,\n",
       "        0.80412371, 0.79591837, 0.72164948, 0.7628866 , 0.78787879,\n",
       "        0.70103093, 0.74226804, 0.7628866 , 0.80808081, 0.82474227,\n",
       "        0.81632653, 0.77083333, 0.78350515, 0.76767677, 0.74226804,\n",
       "        0.7628866 , 0.7755102 , 0.80412371, 0.81632653, 0.7628866 ,\n",
       "        0.74226804, 0.78      , 0.7755102 , 0.7755102 , 0.75789474,\n",
       "        0.78787879, 0.80412371, 0.80412371, 0.7628866 , 0.7755102 ,\n",
       "        0.78787879, 0.76767677, 0.7755102 , 0.75      , 0.7755102 ,\n",
       "        0.73684211, 0.8       , 0.81188119, 0.74747475, 0.76      ,\n",
       "        0.75      , 0.72164948, 0.73469388, 0.75510204, 0.75510204,\n",
       "        0.8       , 0.77083333, 0.74      , 0.7628866 , 0.7628866 ,\n",
       "        0.71428571, 0.74468085, 0.75      , 0.81632653, 0.83673469,\n",
       "        0.82828283, 0.79591837, 0.77083333, 0.78350515, 0.74226804,\n",
       "        0.76595745, 0.75      , 0.79591837, 0.81632653, 0.81632653,\n",
       "        0.78787879, 0.81632653, 0.78787879, 0.76      , 0.75789474,\n",
       "        0.7755102 , 0.79591837, 0.82828283, 0.79591837, 0.78      ,\n",
       "        0.78787879, 0.7755102 , 0.78      , 0.75789474, 0.7628866 ,\n",
       "        0.73684211, 0.8       , 0.79591837, 0.72916667, 0.76767677,\n",
       "        0.77083333, 0.72916667, 0.70833333, 0.73469388, 0.75510204,\n",
       "        0.78350515, 0.8       , 0.72727273, 0.7628866 , 0.75      ,\n",
       "        0.74      , 0.72164948, 0.76595745, 0.7755102 , 0.80412371,\n",
       "        0.80412371, 0.74226804, 0.7628866 , 0.75      , 0.75510204,\n",
       "        0.74226804, 0.75789474, 0.79591837, 0.82      , 0.81632653,\n",
       "        0.78350515, 0.7628866 , 0.78350515, 0.7755102 , 0.75789474,\n",
       "        0.75789474, 0.79207921, 0.82      , 0.82828283, 0.79591837,\n",
       "        0.78      , 0.78787879, 0.76767677, 0.75789474, 0.75789474,\n",
       "        0.73684211, 0.75510204, 0.8       , 0.72340426, 0.73267327,\n",
       "        0.75510204, 0.71578947, 0.72727273, 0.70833333, 0.74226804,\n",
       "        0.78787879, 0.8       , 0.73469388, 0.77227723, 0.78      ,\n",
       "        0.71428571, 0.70707071, 0.71428571, 0.77083333, 0.78787879,\n",
       "        0.8125    , 0.75      , 0.76470588, 0.7755102 , 0.75      ,\n",
       "        0.72164948, 0.75789474, 0.80412371, 0.80808081, 0.80808081,\n",
       "        0.77083333, 0.81188119, 0.8       , 0.75      , 0.7628866 ,\n",
       "        0.74468085, 0.80808081, 0.80392157, 0.82828283, 0.78350515,\n",
       "        0.81188119, 0.8       , 0.75789474, 0.7628866 , 0.74468085,\n",
       "        0.73333333, 0.73684211, 0.73684211, 0.73333333, 0.73684211,\n",
       "        0.73684211, 0.73333333, 0.72340426, 0.72916667, 0.74725275,\n",
       "        0.77083333, 0.75789474, 0.73333333, 0.73684211, 0.75      ,\n",
       "        0.73333333, 0.7628866 , 0.75510204, 0.76595745, 0.77083333,\n",
       "        0.77083333, 0.7311828 , 0.7628866 , 0.7628866 , 0.7173913 ,\n",
       "        0.75      , 0.76767677, 0.77419355, 0.78350515, 0.8       ,\n",
       "        0.76595745, 0.75789474, 0.76767677, 0.70967742, 0.74226804,\n",
       "        0.7628866 , 0.79569892, 0.76767677, 0.79207921, 0.76595745,\n",
       "        0.7755102 , 0.8       , 0.7173913 , 0.7628866 , 0.7755102 ,\n",
       "        0.75510204, 0.82      , 0.8       , 0.73267327, 0.74      ,\n",
       "        0.7628866 , 0.73267327, 0.72727273, 0.74226804, 0.7755102 ,\n",
       "        0.80851064, 0.8       , 0.75247525, 0.7755102 , 0.75510204,\n",
       "        0.74      , 0.73684211, 0.72916667, 0.78      , 0.80412371,\n",
       "        0.78350515, 0.76      , 0.76      , 0.76767677, 0.75247525,\n",
       "        0.75789474, 0.75510204, 0.7755102 , 0.81632653, 0.77894737,\n",
       "        0.76      , 0.76      , 0.78      , 0.76470588, 0.7628866 ,\n",
       "        0.7755102 , 0.79591837, 0.79591837, 0.79166667, 0.76      ,\n",
       "        0.76767677, 0.76      , 0.74747475, 0.75510204, 0.76767677,\n",
       "        0.74226804, 0.79207921, 0.80392157, 0.72727273, 0.76      ,\n",
       "        0.76      , 0.73469388, 0.72      , 0.72727273, 0.74226804,\n",
       "        0.79207921, 0.79166667, 0.72164948, 0.76      , 0.75510204,\n",
       "        0.74747475, 0.73267327, 0.74226804, 0.76470588, 0.79591837,\n",
       "        0.80412371, 0.72727273, 0.75247525, 0.76767677, 0.73469388,\n",
       "        0.75510204, 0.7755102 , 0.78431373, 0.78787879, 0.78350515,\n",
       "        0.79207921, 0.76767677, 0.76767677, 0.76767677, 0.76      ,\n",
       "        0.7755102 , 0.79207921, 0.79591837, 0.78350515, 0.79207921,\n",
       "        0.76767677, 0.77227723, 0.7755102 , 0.76767677, 0.7755102 ,\n",
       "        0.73684211, 0.78431373, 0.8       , 0.74226804, 0.77669903,\n",
       "        0.76470588, 0.73684211, 0.72      , 0.72727273, 0.72164948,\n",
       "        0.78      , 0.8125    , 0.72727273, 0.76470588, 0.74747475,\n",
       "        0.73469388, 0.7254902 , 0.72727273, 0.73469388, 0.78431373,\n",
       "        0.82474227, 0.72727273, 0.74509804, 0.75247525, 0.74226804,\n",
       "        0.74509804, 0.73469388, 0.76767677, 0.8       , 0.79591837,\n",
       "        0.74747475, 0.75247525, 0.7755102 , 0.7628866 , 0.74      ,\n",
       "        0.73469388, 0.76      , 0.78      , 0.80808081, 0.78      ,\n",
       "        0.75247525, 0.76767677, 0.74226804, 0.74      , 0.73469388,\n",
       "        0.74157303, 0.75510204, 0.77227723, 0.72527473, 0.74      ,\n",
       "        0.7961165 , 0.72340426, 0.73469388, 0.73267327, 0.73913043,\n",
       "        0.75247525, 0.77227723, 0.72340426, 0.73267327, 0.76470588,\n",
       "        0.71578947, 0.71287129, 0.7254902 , 0.73684211, 0.77669903,\n",
       "        0.78787879, 0.72916667, 0.76923077, 0.76470588, 0.72340426,\n",
       "        0.73267327, 0.75247525, 0.72916667, 0.76470588, 0.80808081,\n",
       "        0.73684211, 0.75      , 0.76470588, 0.71578947, 0.73267327,\n",
       "        0.74      , 0.75      , 0.76470588, 0.81188119, 0.75      ,\n",
       "        0.77669903, 0.78      , 0.73684211, 0.73267327, 0.74747475,\n",
       "        0.71264368, 0.73333333, 0.72916667, 0.68235294, 0.73333333,\n",
       "        0.72164948, 0.6744186 , 0.72340426, 0.72164948, 0.69565217,\n",
       "        0.71578947, 0.73469388, 0.70967742, 0.72916667, 0.73469388,\n",
       "        0.65934066, 0.73469388, 0.72727273, 0.7032967 , 0.75      ,\n",
       "        0.73469388, 0.70967742, 0.74226804, 0.72727273, 0.68131868,\n",
       "        0.73469388, 0.74747475, 0.7173913 , 0.74226804, 0.74747475,\n",
       "        0.72527473, 0.73684211, 0.75247525, 0.68131868, 0.73469388,\n",
       "        0.75510204, 0.7173913 , 0.74226804, 0.78431373, 0.7173913 ,\n",
       "        0.73684211, 0.76470588, 0.68888889, 0.70833333, 0.76      ]),\n",
       " 'mean_test_score': array([0.77270905, 0.78174977, 0.77606315, 0.77930295, 0.77659997,\n",
       "        0.77937332, 0.76912489, 0.77416567, 0.79063694, 0.77692065,\n",
       "        0.7719423 , 0.76755898, 0.76444581, 0.76888162, 0.77644644,\n",
       "        0.76056297, 0.77963493, 0.78568663, 0.78475881, 0.78062551,\n",
       "        0.77029827, 0.78254969, 0.77691462, 0.77151466, 0.78079255,\n",
       "        0.78293569, 0.77951481, 0.77763788, 0.78021576, 0.7605253 ,\n",
       "        0.77234878, 0.7754902 , 0.77442705, 0.77870438, 0.77915408,\n",
       "        0.77783105, 0.78021605, 0.76809626, 0.75333416, 0.77654865,\n",
       "        0.78138039, 0.7711448 , 0.78571124, 0.78246373, 0.7810718 ,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.77270905, 0.78174977, 0.77606315, 0.77930295, 0.77659997,\n",
       "        0.77937332, 0.76912489, 0.77416567, 0.79063694, 0.77692065,\n",
       "        0.7719423 , 0.76755898, 0.76444581, 0.76888162, 0.77644644,\n",
       "        0.76056297, 0.77963493, 0.78568663, 0.78475881, 0.78062551,\n",
       "        0.77029827, 0.78254969, 0.77691462, 0.77151466, 0.78079255,\n",
       "        0.78293569, 0.77951481, 0.77763788, 0.78021576, 0.7605253 ,\n",
       "        0.77234878, 0.7754902 , 0.77442705, 0.77870438, 0.77915408,\n",
       "        0.77783105, 0.78021605, 0.76809626, 0.75333416, 0.77654865,\n",
       "        0.78138039, 0.7711448 , 0.78571124, 0.78246373, 0.7810718 ,\n",
       "        0.76400538, 0.77963569, 0.7748574 , 0.77634054, 0.78244959,\n",
       "        0.77978884, 0.77448114, 0.77812559, 0.78362676, 0.76759514,\n",
       "        0.77945412, 0.77433054, 0.76540296, 0.77139008, 0.76954606,\n",
       "        0.7665793 , 0.78082263, 0.78408031, 0.77904009, 0.78084834,\n",
       "        0.7843096 , 0.77449226, 0.771915  , 0.78291462, 0.77212864,\n",
       "        0.78796642, 0.78310812, 0.77820051, 0.78211378, 0.77742541,\n",
       "        0.78186942, 0.78181697, 0.78178288, 0.78041738, 0.77424746,\n",
       "        0.79072086, 0.77133165, 0.77860826, 0.76508514, 0.77801947,\n",
       "        0.77486713, 0.77586262, 0.77477228, 0.77974241, 0.780891  ,\n",
       "        0.75725077, 0.77675124, 0.78372599, 0.7643184 , 0.77460189,\n",
       "        0.7829359 , 0.76838147, 0.76620083, 0.77404167, 0.75886795,\n",
       "        0.77508931, 0.77655581, 0.75748404, 0.76955188, 0.77134771,\n",
       "        0.76330664, 0.77348278, 0.77757266, 0.76067092, 0.77821577,\n",
       "        0.77867339, 0.76033081, 0.77373055, 0.75524244, 0.76866257,\n",
       "        0.77448313, 0.77347071, 0.77389288, 0.78118394, 0.77830092,\n",
       "        0.76497955, 0.77539831, 0.77351319, 0.76999083, 0.77453155,\n",
       "        0.77880225, 0.77114756, 0.7803865 , 0.77827597, 0.77383224,\n",
       "        0.7774258 , 0.78213886, 0.77084172, 0.77787213, 0.78253772,\n",
       "        0.76083262, 0.77362357, 0.78303262, 0.75255089, 0.7699803 ,\n",
       "        0.774998  , 0.75462695, 0.7709315 , 0.76529304, 0.75678441,\n",
       "        0.76888048, 0.78513744, 0.75445475, 0.77443061, 0.78004873,\n",
       "        0.75401746, 0.76289874, 0.76958284, 0.76865795, 0.77441424,\n",
       "        0.77859767, 0.757065  , 0.77492675, 0.77351536, 0.76496036,\n",
       "        0.77127452, 0.77254266, 0.77185001, 0.77578108, 0.78250381,\n",
       "        0.76181774, 0.78178742, 0.78323342, 0.76545187, 0.77339977,\n",
       "        0.77089849, 0.77309367, 0.77103591, 0.78458852, 0.76569527,\n",
       "        0.78055344, 0.7782099 , 0.76798231, 0.77397652, 0.76872442,\n",
       "        0.74352236, 0.75035041, 0.76290025, 0.74068548, 0.74898987,\n",
       "        0.76282074, 0.73515667, 0.75283775, 0.75556787, 0.74377398,\n",
       "        0.76097439, 0.76683701, 0.74377713, 0.75388355, 0.76397687,\n",
       "        0.74081685, 0.76005371, 0.7642807 , 0.76182812, 0.76426449,\n",
       "        0.76817553, 0.75738159, 0.76784592, 0.76906558, 0.7426132 ,\n",
       "        0.76006162, 0.7721728 , 0.75965768, 0.76706456, 0.77384996,\n",
       "        0.76762959, 0.75594599, 0.7706418 , 0.74815964, 0.76224953,\n",
       "        0.77009017, 0.76920789, 0.7607462 , 0.76794917, 0.76450788,\n",
       "        0.76305897, 0.77556799, 0.75065068, 0.7649995 , 0.77540772,\n",
       "        0.77690605, 0.78603378, 0.76828496, 0.77347805, 0.77187816,\n",
       "        0.77669004, 0.76745829, 0.76985015, 0.77441755, 0.77268039,\n",
       "        0.77753426, 0.76655036, 0.77057392, 0.77530612, 0.7735639 ,\n",
       "        0.77032385, 0.77076142, 0.77344419, 0.77187569, 0.76704567,\n",
       "        0.76315953, 0.78259786, 0.77265073, 0.77056072, 0.77441784,\n",
       "        0.77916358, 0.78174424, 0.76802709, 0.77654212, 0.76029993,\n",
       "        0.77407283, 0.76845456, 0.77720679, 0.77617927, 0.77981706,\n",
       "        0.77786047, 0.78680788, 0.77063174, 0.76375919, 0.77301245,\n",
       "        0.77283779, 0.77146017, 0.77318925, 0.77109236, 0.77391298,\n",
       "        0.7646251 , 0.78137475, 0.78068832, 0.7549482 , 0.77434219,\n",
       "        0.77676923, 0.76111222, 0.76960539, 0.77061191, 0.75823537,\n",
       "        0.78231699, 0.76890487, 0.75783634, 0.77419637, 0.77277461,\n",
       "        0.76219964, 0.7685917 , 0.77851255, 0.76525796, 0.77145352,\n",
       "        0.7778565 , 0.76434561, 0.77166069, 0.76654815, 0.75953145,\n",
       "        0.77263213, 0.78197406, 0.77257608, 0.77304735, 0.76242354,\n",
       "        0.78224223, 0.76700408, 0.77427823, 0.7685618 , 0.7749526 ,\n",
       "        0.78392864, 0.76491546, 0.7796689 , 0.76211552, 0.78248889,\n",
       "        0.77040402, 0.77628166, 0.77489786, 0.7731734 , 0.78091922,\n",
       "        0.74732092, 0.77968463, 0.78328785, 0.7566251 , 0.77766011,\n",
       "        0.77320946, 0.76065374, 0.76662745, 0.77178074, 0.75344801,\n",
       "        0.77094547, 0.78073407, 0.75154582, 0.77380053, 0.77021857,\n",
       "        0.75894115, 0.76263769, 0.76738024, 0.75044967, 0.76902761,\n",
       "        0.77542296, 0.75309563, 0.77332736, 0.76912502, 0.75560104,\n",
       "        0.76335684, 0.76571429, 0.75804707, 0.77682481, 0.7733171 ,\n",
       "        0.75814001, 0.77061535, 0.76908725, 0.7635709 , 0.7649558 ,\n",
       "        0.76835292, 0.75282748, 0.77256567, 0.77553077, 0.77624133,\n",
       "        0.76610965, 0.77119573, 0.76119564, 0.77205626, 0.76841927,\n",
       "        0.74964794, 0.76995325, 0.77116892, 0.74568053, 0.76517546,\n",
       "        0.78036843, 0.74626107, 0.76874097, 0.76854231, 0.74279775,\n",
       "        0.76495308, 0.77764788, 0.74126867, 0.76506681, 0.7722911 ,\n",
       "        0.73190104, 0.76277599, 0.76919016, 0.74622357, 0.76203069,\n",
       "        0.78188134, 0.74795379, 0.7695356 , 0.77369967, 0.73738939,\n",
       "        0.75567627, 0.77279876, 0.75058119, 0.76415541, 0.77693914,\n",
       "        0.75000962, 0.76524358, 0.77466307, 0.74509245, 0.76047982,\n",
       "        0.7713846 , 0.75331055, 0.77037208, 0.782829  , 0.75082582,\n",
       "        0.77007257, 0.77734099, 0.74930298, 0.76477319, 0.7711433 ,\n",
       "        0.71057533, 0.74807576, 0.74497913, 0.70621506, 0.74832094,\n",
       "        0.75166442, 0.70753681, 0.74952191, 0.75681676, 0.72080438,\n",
       "        0.74041012, 0.75506823, 0.72166857, 0.74228993, 0.75699516,\n",
       "        0.70500313, 0.73642224, 0.75691438, 0.7365074 , 0.74755342,\n",
       "        0.75696772, 0.73871091, 0.74429194, 0.75591456, 0.72186217,\n",
       "        0.74177774, 0.76350946, 0.73743612, 0.74652522, 0.7551943 ,\n",
       "        0.73091412, 0.74448105, 0.7605132 , 0.72199334, 0.74198813,\n",
       "        0.76243455, 0.73181331, 0.75002141, 0.76160435, 0.72689311,\n",
       "        0.74842972, 0.76073338, 0.72238753, 0.7417797 , 0.76865061]),\n",
       " 'std_test_score': array([0.04004678, 0.0454762 , 0.05235505, 0.03959942, 0.05343767,\n",
       "        0.04930868, 0.05262036, 0.05056826, 0.04396742, 0.0373489 ,\n",
       "        0.05103746, 0.0396085 , 0.05487725, 0.03917025, 0.03620764,\n",
       "        0.05322296, 0.04701571, 0.03441973, 0.04975545, 0.04842574,\n",
       "        0.05219392, 0.05606296, 0.0447355 , 0.03372441, 0.05825518,\n",
       "        0.04576475, 0.03306121, 0.04826513, 0.04320016, 0.03954192,\n",
       "        0.05839516, 0.04791444, 0.03475764, 0.05506955, 0.04278924,\n",
       "        0.03837268, 0.05007036, 0.04247152, 0.03593811, 0.04969119,\n",
       "        0.04047696, 0.03579344, 0.05339348, 0.03815429, 0.03211246,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.04004678, 0.0454762 , 0.05235505, 0.03959942, 0.05343767,\n",
       "        0.04930868, 0.05262036, 0.05056826, 0.04396742, 0.0373489 ,\n",
       "        0.05103746, 0.0396085 , 0.05487725, 0.03917025, 0.03620764,\n",
       "        0.05322296, 0.04701571, 0.03441973, 0.04975545, 0.04842574,\n",
       "        0.05219392, 0.05606296, 0.0447355 , 0.03372441, 0.05825518,\n",
       "        0.04576475, 0.03306121, 0.04826513, 0.04320016, 0.03954192,\n",
       "        0.05839516, 0.04791444, 0.03475764, 0.05506955, 0.04278924,\n",
       "        0.03837268, 0.05007036, 0.04247152, 0.03593811, 0.04969119,\n",
       "        0.04047696, 0.03579344, 0.05339348, 0.03815429, 0.03211246,\n",
       "        0.03342125, 0.04671311, 0.05035899, 0.03967073, 0.04845098,\n",
       "        0.04556326, 0.04921518, 0.05113133, 0.0476812 , 0.04344504,\n",
       "        0.04752719, 0.04006319, 0.05145488, 0.05407431, 0.04246104,\n",
       "        0.05245849, 0.05087268, 0.0445761 , 0.06453408, 0.06264225,\n",
       "        0.05533205, 0.06434057, 0.04361894, 0.03823989, 0.05176992,\n",
       "        0.05144846, 0.04938575, 0.05948372, 0.04943102, 0.04693404,\n",
       "        0.05439546, 0.05216075, 0.04083384, 0.04963371, 0.04710256,\n",
       "        0.03336942, 0.06332203, 0.05183794, 0.04171809, 0.04857362,\n",
       "        0.04594921, 0.04255765, 0.05444399, 0.0417523 , 0.03935366,\n",
       "        0.03935172, 0.0495708 , 0.04146004, 0.04274684, 0.05152798,\n",
       "        0.04450407, 0.04398401, 0.05180568, 0.05239356, 0.03869535,\n",
       "        0.05427629, 0.05372103, 0.03494889, 0.05068547, 0.04965832,\n",
       "        0.03446874, 0.05995587, 0.05119433, 0.05692732, 0.0535461 ,\n",
       "        0.05268854, 0.05412272, 0.04952217, 0.04278299, 0.04231923,\n",
       "        0.05576307, 0.04587463, 0.05502193, 0.05545975, 0.04920663,\n",
       "        0.06404778, 0.05354081, 0.04940963, 0.04891942, 0.05245051,\n",
       "        0.04398141, 0.06026391, 0.05065318, 0.05122161, 0.06115906,\n",
       "        0.04168308, 0.0461709 , 0.04719052, 0.05387559, 0.0380083 ,\n",
       "        0.0387105 , 0.03843047, 0.03287785, 0.05730716, 0.04698021,\n",
       "        0.051845  , 0.04673031, 0.04747127, 0.05369581, 0.04587757,\n",
       "        0.04880264, 0.04590226, 0.04670411, 0.04304487, 0.05020125,\n",
       "        0.0492211 , 0.0518866 , 0.06234154, 0.04876816, 0.05921006,\n",
       "        0.05599441, 0.05017324, 0.05216032, 0.05724539, 0.0498344 ,\n",
       "        0.05631358, 0.05909398, 0.05578056, 0.06386929, 0.05048987,\n",
       "        0.06272338, 0.06556465, 0.05656317, 0.04355977, 0.05108064,\n",
       "        0.05947658, 0.0532833 , 0.05735701, 0.04954849, 0.05976983,\n",
       "        0.062939  , 0.05325115, 0.0477797 , 0.05196764, 0.05494501,\n",
       "        0.04569593, 0.05266616, 0.04951622, 0.04975833, 0.0546839 ,\n",
       "        0.04747507, 0.0450791 , 0.04979544, 0.04467616, 0.04484343,\n",
       "        0.04796101, 0.05256453, 0.05212687, 0.0552884 , 0.04681725,\n",
       "        0.04090497, 0.05457818, 0.04317195, 0.04857181, 0.05181057,\n",
       "        0.056115  , 0.05281594, 0.05233868, 0.04977414, 0.03517592,\n",
       "        0.05347302, 0.044551  , 0.04412023, 0.04740625, 0.05488853,\n",
       "        0.05862601, 0.05559632, 0.04862756, 0.05230754, 0.05319284,\n",
       "        0.05167531, 0.05520563, 0.05262435, 0.05596161, 0.05833895,\n",
       "        0.05448051, 0.04907981, 0.04567519, 0.05439382, 0.05087884,\n",
       "        0.03468459, 0.04731971, 0.05669695, 0.05112019, 0.05627722,\n",
       "        0.05067905, 0.05101685, 0.05418655, 0.05325322, 0.04361997,\n",
       "        0.04450892, 0.04705291, 0.05086977, 0.05399647, 0.0474307 ,\n",
       "        0.04596631, 0.05849607, 0.04909612, 0.04950388, 0.04464262,\n",
       "        0.03334699, 0.0496852 , 0.04223026, 0.03221618, 0.05397928,\n",
       "        0.04964871, 0.03904538, 0.0470693 , 0.04964067, 0.03202918,\n",
       "        0.06598135, 0.05112595, 0.0444751 , 0.04686917, 0.051146  ,\n",
       "        0.04507871, 0.04858394, 0.04427648, 0.04180495, 0.06296765,\n",
       "        0.051426  , 0.05227151, 0.0540804 , 0.0423042 , 0.03825025,\n",
       "        0.03816129, 0.03518592, 0.04075258, 0.04914624, 0.04776831,\n",
       "        0.04941648, 0.04775401, 0.06341077, 0.05496931, 0.04958089,\n",
       "        0.04605519, 0.04897918, 0.04973751, 0.04176968, 0.04776757,\n",
       "        0.04459878, 0.06036219, 0.05310885, 0.05729076, 0.04813312,\n",
       "        0.04245504, 0.05436377, 0.0447545 , 0.04784204, 0.04860457,\n",
       "        0.05297003, 0.05570314, 0.0567386 , 0.04155167, 0.04368582,\n",
       "        0.05584914, 0.04322847, 0.03903681, 0.05219343, 0.05232356,\n",
       "        0.03616148, 0.06237612, 0.04188921, 0.03925779, 0.05371559,\n",
       "        0.0483467 , 0.04854556, 0.06032737, 0.05695128, 0.04513514,\n",
       "        0.04627783, 0.03721314, 0.0337867 , 0.05666147, 0.04448453,\n",
       "        0.05136025, 0.05026203, 0.04815016, 0.05320229, 0.05318021,\n",
       "        0.04538596, 0.04926359, 0.05646089, 0.05034035, 0.04468789,\n",
       "        0.04070753, 0.05619731, 0.05453729, 0.05031366, 0.04939743,\n",
       "        0.0605145 , 0.04866982, 0.0467345 , 0.04456927, 0.04236491,\n",
       "        0.04909074, 0.05427463, 0.05603018, 0.04673237, 0.04422357,\n",
       "        0.05431653, 0.04320362, 0.04282084, 0.04508954, 0.05897238,\n",
       "        0.05370971, 0.06357209, 0.0424579 , 0.0479248 , 0.05721302,\n",
       "        0.05175175, 0.04258794, 0.05030686, 0.06074379, 0.05184144,\n",
       "        0.05082291, 0.02993929, 0.03830383, 0.06216933, 0.04570842,\n",
       "        0.04792113, 0.04321683, 0.04409874, 0.04765355, 0.04998556,\n",
       "        0.04986715, 0.04487354, 0.05259948, 0.04792679, 0.04964042,\n",
       "        0.03950308, 0.0538643 , 0.05204479, 0.0531972 , 0.0538901 ,\n",
       "        0.05058668, 0.05720173, 0.04793628, 0.05494283, 0.03815762,\n",
       "        0.04771691, 0.05787219, 0.05644809, 0.04616912, 0.04741514,\n",
       "        0.05184756, 0.05616092, 0.0484463 , 0.04391026, 0.05364264,\n",
       "        0.05802794, 0.05058097, 0.04882764, 0.04886972, 0.05763106,\n",
       "        0.05447267, 0.05389363, 0.04186004, 0.05448764, 0.05680812,\n",
       "        0.0597256 , 0.05028869, 0.04754808, 0.06223935, 0.05289919,\n",
       "        0.05121671, 0.05800556, 0.04804207, 0.0533574 , 0.05196918,\n",
       "        0.0513209 , 0.05642011, 0.06001586, 0.05174424, 0.05536379,\n",
       "        0.05345758, 0.03865305, 0.04059513, 0.06837951, 0.05382093,\n",
       "        0.05107438, 0.05943977, 0.0527892 , 0.05202275, 0.04393994,\n",
       "        0.0376682 , 0.0427533 , 0.05519104, 0.04892661, 0.04920166,\n",
       "        0.0504471 , 0.04843146, 0.05236624, 0.0429899 , 0.0297756 ,\n",
       "        0.03491666, 0.056032  , 0.05486682, 0.04772114, 0.052434  ,\n",
       "        0.04923666, 0.05087227, 0.04760869, 0.0398153 , 0.04798134]),\n",
       " 'rank_test_score': array([301,  70, 199, 123, 185, 121, 393, 256,   3, 175, 318, 436, 482,\n",
       "        400, 192, 537, 115,  11,  15,  91, 369,  43, 177, 328,  87,  37,\n",
       "        117, 163, 102, 539, 310, 208, 243, 132, 126, 159, 100, 423, 596,\n",
       "        189,  73, 345,   9,  50,  78, 492, 113, 226, 194,  52, 107, 239,\n",
       "        151,  26, 434, 119, 250, 459, 332, 386, 447,  85,  21, 128,  83,\n",
       "         19, 235, 320,  39, 315,   5,  31, 149,  58, 170,  62,  64,  68,\n",
       "         95, 253,   1, 337, 136, 466, 153, 224, 201, 228, 109,  81, 564,\n",
       "        182,  24, 485, 231,  35, 417, 451, 259, 554, 216, 187, 560, 384,\n",
       "        335, 500, 281, 165, 534, 145, 134, 543, 271, 582, 407, 237, 284,\n",
       "        264,  76, 141, 471, 213, 279, 375, 233, 130, 343,  97, 143, 268,\n",
       "        168,  56, 356, 155,  45, 529, 274,  33, 603, 377, 218, 587, 352,\n",
       "        461, 572, 402,  13, 589, 241, 104, 591, 507, 382, 409, 247, 138,\n",
       "        566, 221, 277, 473, 339, 308, 324, 203,  47, 522,  66,  29, 457,\n",
       "        287, 354, 294, 349,  17, 455,  93, 147, 426, 261, 405, 641, 612,\n",
       "        505, 653, 619, 509, 661, 600, 580, 639, 527, 444, 637, 593, 494,\n",
       "        651, 548, 487, 520, 489, 421, 562, 430, 396, 644, 546, 313, 550,\n",
       "        440, 266, 432, 575, 359, 623, 515, 372, 389, 531, 428, 480, 503,\n",
       "        205, 608, 469, 211, 301,  70, 199, 123, 185, 121, 393, 256,   3,\n",
       "        175, 318, 436, 482, 400, 192, 537, 115,  11,  15,  91, 369,  43,\n",
       "        177, 328,  87,  37, 117, 163, 102, 539, 310, 208, 243, 132, 126,\n",
       "        159, 100, 423, 596, 189,  73, 345,   9,  50,  78, 492, 113, 226,\n",
       "        194,  52, 107, 239, 151,  26, 434, 119, 250, 459, 332, 386, 447,\n",
       "         85,  21, 128,  83,  19, 235, 320,  39, 315,   5,  31, 149,  58,\n",
       "        170,  62,  64,  68,  95, 253,   1, 337, 136, 466, 153, 224, 201,\n",
       "        228, 109,  81, 564, 182,  24, 485, 231,  35, 417, 451, 259, 554,\n",
       "        216, 187, 560, 384, 335, 500, 281, 165, 534, 145, 134, 543, 271,\n",
       "        582, 407, 237, 284, 264,  76, 141, 471, 213, 279, 375, 233, 130,\n",
       "        343,  97, 143, 268, 168,  56, 356, 155,  45, 529, 274,  33, 603,\n",
       "        377, 218, 587, 352, 461, 572, 402,  13, 589, 241, 104, 591, 507,\n",
       "        382, 409, 247, 138, 566, 221, 277, 473, 339, 308, 324, 203,  47,\n",
       "        522,  66,  29, 457, 287, 354, 294, 349,  17, 455,  93, 147, 426,\n",
       "        261, 405, 641, 612, 505, 653, 619, 509, 661, 600, 580, 639, 527,\n",
       "        444, 637, 593, 494, 651, 548, 487, 520, 489, 421, 562, 430, 396,\n",
       "        644, 546, 313, 550, 440, 266, 432, 575, 359, 623, 515, 372, 389,\n",
       "        531, 428, 480, 503, 205, 608, 469, 211, 179,   8, 420, 283, 322,\n",
       "        184, 438, 380, 246, 303, 167, 449, 364, 215, 276, 368, 358, 286,\n",
       "        323, 442, 502,  42, 304, 365, 245, 125,  72, 425, 191, 545, 258,\n",
       "        415, 173, 198, 106, 157,   7, 361, 496, 297, 298, 330, 292, 348,\n",
       "        263, 479,  75,  90, 586, 249, 181, 526, 381, 363, 556,  54, 399,\n",
       "        559, 255, 300, 517, 412, 140, 463, 331, 158, 484, 327, 450, 552,\n",
       "        305,  60, 306, 296, 514,  55, 443, 252, 413, 220,  23, 477, 112,\n",
       "        518,  49, 366, 196, 223, 293,  80, 628, 111,  28, 574, 161, 291,\n",
       "        536, 446, 326, 595, 351,  89, 606, 270, 371, 553, 512, 439, 611,\n",
       "        398, 210, 599, 289, 392, 579, 499, 454, 558, 180, 290, 557, 362,\n",
       "        395, 497, 475, 419, 602, 307, 207, 197, 453, 341, 525, 317, 416,\n",
       "        616, 379, 342, 632, 465,  99, 630, 404, 414, 643, 476, 162, 650,\n",
       "        468, 312, 663, 511, 391, 631, 519,  61, 626, 388, 273, 658, 578,\n",
       "        299, 610, 491, 174, 615, 464, 230, 633, 542, 334, 598, 367,  41,\n",
       "        607, 374, 172, 618, 478, 347, 672, 625, 634, 674, 622, 605, 673,\n",
       "        617, 571, 671, 655, 585, 670, 646, 568, 675, 660, 570, 659, 627,\n",
       "        569, 656, 636, 577, 669, 649, 498, 657, 629, 584, 665, 635, 541,\n",
       "        668, 647, 513, 664, 614, 524, 666, 621, 533, 667, 648, 411],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.07,\n",
       " 'max_depth': 6,\n",
       " 'min_child_weight': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907208606102268"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.775785\n",
      "F1: 0.652778\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c1bfa60d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 231,\n",
       " 'Parch': 91,\n",
       " 'male': 42,\n",
       " 'Pclass': 81,\n",
       " 'SibSp': 86,\n",
       " 'S': 43,\n",
       " 'Q': 37,\n",
       " 'youngin': 27,\n",
       " 'Age': 150}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=3, missing=nan, n_estimators=500, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
